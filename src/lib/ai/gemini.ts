import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from '@google/generative-ai';

import { Message } from '../store';
import { PromptManager } from '../engine/prompt-manager';
import { LoreConverter } from '../utils/lore-converter';
import { getLogicPrompt, getStaticLogicPrompt, getDynamicLogicPrompt } from '@/data/prompts/logic';
import { MODEL_CONFIG, calculateCost } from './model-config';
import { EventManager } from '../engine/event-manager';

// Removed static SYSTEM_PROMPT in favor of dynamic generation

const safetySettings = [
    {
        category: HarmCategory.HARM_CATEGORY_HARASSMENT,
        threshold: HarmBlockThreshold.BLOCK_NONE,
    },
    {
        category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold: HarmBlockThreshold.BLOCK_NONE,
    },
    {
        category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold: HarmBlockThreshold.BLOCK_NONE,
    },
    {
        category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold: HarmBlockThreshold.BLOCK_NONE,
    },
];

// Helper: Retry with Exponential Backoff
async function retryWithBackoff<T>(
    fn: () => Promise<T>,
    retries: number = 3,
    delay: number = 1000,
    operationName: string = 'Gemini API'
): Promise<T> {
    try {
        return await fn();
    } catch (error: any) {
        if (retries === 0 || !shouldRetry(error)) throw error;

        console.warn(`[Gemini] ${operationName} failed. Retrying... (${retries} attempts left). Error: ${error.message}`);
        await new Promise(resolve => setTimeout(resolve, delay));
        return retryWithBackoff(fn, retries - 1, delay * 2, operationName);
    }
}

function shouldRetry(error: any): boolean {
    const msg = (error.message || '').toLowerCase();
    // 500: Internal Server Error
    // 503: Service Unavailable
    // 429: Too Many Requests (Rate Limit) -> Wait longer? Default backoff helps.
    // fetch failed: Network issues
    return msg.includes('500') || msg.includes('503') || msg.includes('429') || msg.includes('internal error') || msg.includes('fetch failed') || msg.includes('overloaded');
}

// [NEW] Helper: Get or Create Cache
// Gemini requires minimal 32k tokens for caching. Standard logic prompts are too small (~5k).
// Only Story Prompt (~35k) validates for caching.
// [Refactor] Dynamic Import Wrapper
async function getOrUpdateCache(
    apiKey: string,
    cacheKey: string,
    systemInstruction: string,
    modelName: string
): Promise<string | null> {
    // 1. Environment Check
    if (typeof window !== 'undefined') {
        console.warn("[GeminiCache] Attempted to access CacheManager from Client Side. Skipping.");
        return null; // Client side cannot cache
    }

    try {
        // 2. Dynamic Import of Server-Only Module
        // @ts-ignore - Build time check bypass
        const { getOrUpdateCache: serverGetOrUpdateCache } = await import('./gemini-server');
        return await serverGetOrUpdateCache(apiKey, cacheKey, systemInstruction, modelName);

    } catch (error) {
        console.error("[GeminiCache] Failed to load server module:", error);
        return null;
    }
}

export async function generateResponse(
    apiKey: string,
    history: Message[],
    userMessage: string,
    gameState: any, // Pass the full game state to generate the prompt
    language: 'ko' | 'en' | null,
    modelName: string = MODEL_CONFIG.STORY // [NEW] Accept model override, default to config
) {
    if (!apiKey) throw new Error('API Key is missing');

    console.log(`[Gemini] generateResponse called. Model: ${modelName}. gameState.playerStats.luk: ${gameState.playerStats?.luk}`);

    const genAI = new GoogleGenerativeAI(apiKey);

    // Generate dynamic system prompt based on current game state

    // [SERVER REHYDRATION FIX]
    // If getSystemPromptTemplate is missing (because functions don't survive network serialization),
    // and we have activeGameId, we try to reload it dynamically.
    if (!gameState.getSystemPromptTemplate && gameState.activeGameId) {
        console.log(`[Gemini] Rehydrating System Prompt for game: ${gameState.activeGameId}`);
        try {
            // Dynamic import based on game ID
            const systemModule = await import(`@/data/games/${gameState.activeGameId}/prompts/system`);
            if (systemModule && systemModule.getSystemPromptTemplate) {
                gameState.getSystemPromptTemplate = systemModule.getSystemPromptTemplate;
            }
        } catch (e) {
            console.warn(`[Gemini] Failed to rehydrate system prompt for ${gameState.activeGameId}:`, e);
        }
    }

    const staticPrompt = await PromptManager.getSharedStaticContext(gameState);
    const dynamicPrompt = PromptManager.generateSystemPrompt(gameState, language, userMessage);

    // [DEBUG] Check Lore Injection
    if (gameState.lore) {
        // console.log(`[Gemini] Lore Data Present. Keys: ${Object.keys(gameState.lore).join(', ')}`);
    } else {
        console.warn("[Gemini] âš ï¸ Lore Data is MISSING in gameState!");
    }

    // [ì»¨í…ìŠ¤íŠ¸ ìºì‹± í‚¤ ìˆ˜ì •]
    // ì •ì (Static) í”„ë¡¬í”„íŠ¸ì™€ ë™ì (Dynamic) í”„ë¡¬í”„íŠ¸ë¥¼ í•©ì¹˜ì§€ ë§ˆì„¸ìš”.
    // Static = systemInstruction (ìºì‹œë¨, ë¹„ìš© ì ˆê°)
    // Dynamic = ìœ ì € ë©”ì‹œì§€ì˜ ì¼ë¶€ (ìºì‹œ ì•ˆë¨, ë§¤ë²ˆ ë³€ê²½)
    const systemInstruction = staticPrompt;

    // [Fix] Use passed modelName if available, otherwise default to config
    // Also ensuring no variable shadowing in loop
    const targetModel = modelName || MODEL_CONFIG.STORY;

    // [CACHE IMPLEMENTATION - STORY MODEL ONLY]
    // We try to get a valid cache name.
    let cachedContentName: string | null = null;

    // Determine the Cache Key: Same logic as PromptManager but cleaner
    const overrideKey = gameState.personaOverride ? `_PERSONA_${gameState.personaOverride}` : '';
    // Use a simpler display name that matches PromptManager's logic but safer for display name (no spaces if possible, though Gemini allows)
    // PromptManager Key: PROMPT_CACHE_{GameID}_SHARED_{Version}
    // We reuse PromptManager's public method logic if possible, or replicate it.
    // Let's replicate the key structure: `CACHE_{GameID}_STORY_{Version}`
    // [Fix] Cache Mismatch Error (400): Model used by GenerateContent (Flash) and CachedContent (Pro) must be same.
    // Solution: Include Model Name in Key.
    const sanitizedModelName = targetModel.replace(/[^a-zA-Z0-9]/g, '_').toUpperCase();
    const cacheDisplayName = `CACHE_${gameState.activeGameId}_STORY_v2_0_${sanitizedModelName}${overrideKey}`;

    // Only attempt caching if the prompt is substantial (Static Prompt is huge)
    // And only for Story models (Logic is too small)
    console.log(`[GeminiCache] Static Prompt Length: ${systemInstruction.length} chars.`);

    if (systemInstruction.length > 25000) { // Lowered to 30k chars as safety margin for 32k tokens (which is roughly ~100k chars? No, tokens > chars/4 usually? Korean is distinct.)
        // Actually 1 token ~= 1.5-3 chars for Korean? 
        // 32k tokens * 2 chars = 64k chars. 
        // If 34k tokens total input, and static is 90% of it, it should be huge.
        // Let's rely on the server to reject if too small.
        console.log(`[GeminiCache] Attempting to retrieve/create cache for key: ${cacheDisplayName}`);
        cachedContentName = await getOrUpdateCache(apiKey, cacheDisplayName, systemInstruction, targetModel);
        console.log(`[GeminiCache] getOrUpdateCache Result: ${cachedContentName}`);
    } else {
        console.log(`[GeminiCache] Static Prompt length (${systemInstruction.length} chars) below threshold (30000). Skipping Cache.`);
    }

    const modelsToTry = [
        targetModel,
        MODEL_CONFIG.LOGIC, // Fallback
    ];

    let lastError;

    // Rename loop variable to avoid shadowing arg 'modelName'
    for (const currentModel of modelsToTry) {
        try {
            console.log(`Trying story model: ${currentModel}`);

            const modelConfig: any = {
                model: currentModel,
                safetySettings
            };

            // [Gemini 3 ìµœì í™”] Native Thinking í™œì„±í™”
            // [Fix] Flash ëª¨ë¸ì€ ì†ë„ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ Thinkingì„ ë¹„í™œì„±í™”í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
            // ëª…ì‹œì ì¸ 'thinking' ëª¨ë¸ì´ê±°ë‚˜, 'gemini-3' ì´ë©´ì„œ 'flash'ê°€ ì•„ë‹Œ ê²½ìš°(Pro ë“±)ì—ë§Œ í™œì„±í™”
            if (currentModel.includes('thinking') || (currentModel.includes('gemini-3') && !currentModel.includes('flash'))) {
                modelConfig.thinkingConfig = {
                    includeThoughts: true, // [User Request] ìƒê° ê³¼ì • ë¡œê·¸ í™•ì¸ì„ ìœ„í•´ True ì„¤ì •
                    thinkingLevel: "high"
                };
            }

            // [CRITICAL] CACHE INJECTION
            // If we have a valid cache, we supply 'cachedContent' and REMOVE 'systemInstruction'.
            // The SDK pattern for cached content:
            if (cachedContentName && currentModel === targetModel) { // Only use cache for the target model it was created for
                console.log(`[Gemini] USING CACHED CONTENT: ${cachedContentName}`);
                modelConfig.cachedContent = { name: cachedContentName };
                // systemInstruction MUST BE REMOVED if cachedContent is present? 
                // Actually, the cache *contains* the systemInstruction. 
                // We should NOT pass it again as 'systemInstruction' property if it's in cache.
            } else {
                console.log(`[Gemini] Using Standard System Instruction (No Cache)`);
                modelConfig.systemInstruction = systemInstruction;
            }

            // [DYNAMIC MODEL] Override model if provided (already handled by logic above, ensuring key matches)
            // But if we are falling back to LOGIC model in the loop, we shouldn't use the Story Cache (model mismatch)
            if (currentModel !== targetModel && modelConfig.cachedContent) {
                delete modelConfig.cachedContent;
                modelConfig.systemInstruction = systemInstruction; // Restore system prompt for fallback
                modelConfig.model = currentModel;
            }


            const model = genAI.getGenerativeModel(modelConfig);

            // [ìµœì í™”]
            // ìš”ì•½ë³¸ì´ ì¡´ì¬í•œë‹¤ë©´ ì „ì²´ íˆìŠ¤í† ë¦¬ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
            // ìµœê·¼ ëŒ€í™”(ì˜ˆ: ë§ˆì§€ë§‰ 10ê°œ ë©”ì‹œì§€ = 5í„´)ë§Œ ë‚¨ê²¨ í† í°ì„ ì ˆì•½í•˜ê³ 
            // ìš”ì•½ë³¸ê³¼ ì „ì²´ íˆìŠ¤í† ë¦¬ê°€ ì¤‘ë³µë˜ì–´ ì •ë³´ê°€ ì™œê³¡ë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
            let effectiveHistory = history;
            if (gameState.scenarioSummary && gameState.scenarioSummary.length > 50) {
                // ë§ˆì§€ë§‰ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
                effectiveHistory = history.slice(-10);
            }

            // Gemini APIë¥¼ ìœ„í•´ íˆìŠ¤í† ë¦¬ í¬ë§·íŒ… (ë°˜ë“œì‹œ 'user'ë¡œ ì‹œì‘í•´ì•¼ í•¨)
            let processedHistory = effectiveHistory.map(msg => ({
                role: msg.role,
                parts: [{ text: msg.text || "..." }],
            }));

            // ìˆ˜ì • 1: í”„ë¡œí† ì½œì€ 'user'ì™€ 'model'ì´ ë°˜ë“œì‹œ ë²ˆê°ˆì•„ ë‚˜ì™€ì•¼ í•˜ë©°,ì‘ì€ 'user'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
            // ë§Œì•½ íˆìŠ¤í† ë¦¬ê°€ 'model'ë¡œ ì‹œì‘í•œë‹¤ë©´, ë”ë¯¸ 'user' ë©”ì‹œì§€ë¥¼ ì•ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            if (processedHistory.length > 0 && processedHistory[0].role === 'model') {
                processedHistory = [
                    { role: 'user', parts: [{ text: "..." }] }, // ë”ë¯¸ ì»¨í…ìŠ¤íŠ¸ ì‹œì‘
                    ...processedHistory
                ];
            }

            const chatSession = model.startChat({
                history: processedHistory,
            });

            // [ìˆ˜ì •] ë™ì  í”„ë¡¬í”„íŠ¸(ê¸°ë¶„, ìƒíƒœì°½ ë“±)ë¥¼ ìœ ì € ë©”ì‹œì§€ ë‚´ë¶€ì— ì£¼ì…í•©ë‹ˆë‹¤.
            // ì´ë ‡ê²Œ í•˜ë©´ ì •ì  ìºì‹œ(Static Cache)ë¥¼ ê¹¨ì§€ ì•Šìœ¼ë©´ì„œë„ ë§¤ í„´ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            const finalUserMessage = `${dynamicPrompt}\n\n${userMessage}`;

            // [Retry Wrapper]
            const result = await retryWithBackoff(
                () => chatSession.sendMessage(finalUserMessage),
                3,
                1500,
                `Story Generation (${currentModel})`
            );
            const response = result.response;

            // [DEBUG] Log Thinking Process
            // Gemini 3.0 Thinking Model returns thoughts in candidate parts
            const parts = response.candidates?.[0]?.content?.parts || [];
            // Note: The structure might vary, but usually thoughts are tagged or separate.
            // We just log all parts that are NOT the final text if possible, or log everything for inspection.
            // Standard 'text()' method filters out thoughts usually? Let's verify by logging.
            console.log("--- [Gemini Thinking Process] ---");
            parts.forEach((p: any, idx: number) => {
                // Check for thought property (might be specific to SDK version) or just log content
                // If p.thought is true, or check text content
                if (p.thought || (idx === 0 && parts.length > 1)) {
                    console.log(`Thought Part ${idx}:`, p.text || p);
                }
            });
            console.log("---------------------------------");

            return {
                text: response.text(),
                usageMetadata: response.usageMetadata,
                systemPrompt: systemInstruction, // "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"ë¡œ ì •ì  ë¶€ë¶„ ë¡œê·¸
                finalUserMessage: finalUserMessage, // [DEBUG] Return Actual Dynamic Input
                usedModel: currentModel,
                totalTokenCount: (response.usageMetadata?.promptTokenCount || 0) + (response.usageMetadata?.candidatesTokenCount || 0)
            };
        } catch (error: any) {
            console.error(`Story Model ${currentModel} failed:`, error.message || error);
            lastError = error;
            // Continue to next model in list
        }
    }

    throw lastError || new Error("All story models failed.");
}

// [STREAMING IMPLEMENTATION]
// Identical logic to generateResponse, but uses sendMessageStream and returns a generator
export async function* generateResponseStream(
    apiKey: string,
    history: Message[],
    userMessage: string,
    gameState: any,
    language: 'ko' | 'en' | null,
    modelName: string = MODEL_CONFIG.STORY
): AsyncGenerator<string | { usageMetadata: any, systemPrompt: string, finalUserMessage: string, usedModel: string, totalTokenCount: number }, void, unknown> {
    if (!apiKey) throw new Error('API Key is missing');

    console.log(`[GeminiStream] generateResponseStream called. Model: ${modelName}`);

    const genAI = new GoogleGenerativeAI(apiKey);

    // [Rehydration Check]
    if (!gameState.getSystemPromptTemplate && gameState.activeGameId) {
        try {
            const systemModule = await import(`@/data/games/${gameState.activeGameId}/prompts/system`);
            if (systemModule && systemModule.getSystemPromptTemplate) {
                gameState.getSystemPromptTemplate = systemModule.getSystemPromptTemplate;
            }
        } catch (e) { console.warn("Stream Rehydration failed", e); }
    }

    const staticPrompt = await PromptManager.getSharedStaticContext(gameState);
    const dynamicPrompt = PromptManager.generateSystemPrompt(gameState, language, userMessage);
    const systemInstruction = staticPrompt;
    const targetModel = modelName || MODEL_CONFIG.STORY;

    // [Cache Logic]
    let cachedContentName: string | null = null;
    const overrideKey = gameState.personaOverride ? `_PERSONA_${gameState.personaOverride}` : '';
    // [Fix] Cache Mismatch Error (400): Include Model Name in Key
    const sanitizedModelName = targetModel.replace(/[^a-zA-Z0-9]/g, '_').toUpperCase();
    const cacheDisplayName = `CACHE_${gameState.activeGameId}_STORY_v2_0_${sanitizedModelName}${overrideKey}`;

    // [DEBUG: CACHE DISABLED] Threshold raised to 35000 to force NO CACHE (Reverting to "Yesterday" state)
    if (systemInstruction.length > 25000) {
        cachedContentName = await getOrUpdateCache(apiKey, cacheDisplayName, systemInstruction, targetModel);
    }

    const modelsToTry = [targetModel, MODEL_CONFIG.LOGIC];
    let lastError;

    for (const currentModel of modelsToTry) {
        try {
            console.log(`Trying stream story model: ${currentModel}`);
            const modelConfig: any = { model: currentModel, safetySettings };

            if (currentModel.includes('thinking') || (currentModel.includes('gemini-3') && !currentModel.includes('flash'))) {
                modelConfig.thinkingConfig = { includeThoughts: true, thinkingLevel: "high" };
            }

            if (cachedContentName && currentModel === targetModel) {
                modelConfig.cachedContent = { name: cachedContentName };
            } else {
                modelConfig.systemInstruction = systemInstruction;
            }

            if (currentModel !== targetModel && modelConfig.cachedContent) {
                delete modelConfig.cachedContent;
                modelConfig.systemInstruction = systemInstruction;
                modelConfig.model = currentModel;
            }

            const model = genAI.getGenerativeModel(modelConfig, { apiVersion: 'v1beta' });

            // [DEBUG] Log History Size
            console.log(`[GeminiStream] History Size: ${history.length} messages.`);

            // History Filtering
            let effectiveHistory = history;
            // [Adjustment] Increase Context Window from 5 to 15 messages (approx 7 turns)
            // 5 messages (2.5 turns) is too short for immediate context even with summary.
            // 15 messages provides a better buffer for "recent conversation".
            effectiveHistory = history.slice(-15);
            console.log(`[GeminiStream] Formatted History. Using last ${effectiveHistory.length} messages.`);

            // [Optimization] Gemini SDK requires alternating roles. 
            let processedHistory = effectiveHistory.map(msg => ({
                role: msg.role === 'user' ? 'user' : 'model',
                parts: [{ text: msg.text || "..." }],
            }));

            // Only prepend dummy if strictly necessary
            if (processedHistory.length > 0 && processedHistory[0].role === 'model') {
                processedHistory = [{ role: 'user', parts: [{ text: "(Context Continuation)" }] }, ...processedHistory];
            }

            console.log(`[GeminiStream] Final Logic/Model Config:`, JSON.stringify(modelConfig, null, 2));

            const chatSession = model.startChat({ history: processedHistory });
            const finalUserMessage = `${dynamicPrompt}\n\n${userMessage}`;

            // STREAMING CALL
            console.log(`[GeminiStream] Sending message to API... (User Msg: ${finalUserMessage.length} chars)`);
            const streamStartTime = Date.now();
            const result = await chatSession.sendMessageStream(finalUserMessage);
            console.log(`[GeminiStream] Stream object received. Waiting for first chunk...`);

            let accumulatedText = "";
            let firstChunkReceived = false;
            let chunkCount = 0;

            for await (const chunk of result.stream) {
                chunkCount++;
                const now = Date.now();
                if (!firstChunkReceived) {
                    const ttft = now - streamStartTime;
                    console.log(`[GeminiStream] ğŸš€ FIRST CHUNK returned from iterator after ${ttft}ms (TTFT)`);
                    firstChunkReceived = true;
                }

                // [Debug] Chunk inspection
                // console.log(`[GeminiStream] Chunk #${chunkCount}: Candidates=${chunk.candidates?.length}`);

                let chunkText = '';
                try {
                    chunkText = chunk.text();
                } catch (e) {
                    // console.log(`[GeminiStream] Chunk #${chunkCount} has no text:`, e);
                }

                // Handle thoughts if present but no text
                if (!chunkText && chunk.candidates && chunk.candidates.length > 0) {
                    // Check candidates for parts
                    /* 
                    const parts = chunk.candidates[0].content?.parts || [];
                    parts.forEach(p => {
                       // if (p.thought) console.log(`[GeminiStream] THOUGHT:`, p.text);
                    });
                    */
                }

                if (chunkText) {
                    accumulatedText += chunkText;
                    yield chunkText; // Yield partial text
                }
            }

            const response = await result.response; // Wait for full completion to get metadata

            // Yield Final Metadata Object as the last item
            yield {
                usageMetadata: response.usageMetadata,
                systemPrompt: systemInstruction,
                finalUserMessage: finalUserMessage,
                usedModel: currentModel,
                totalTokenCount: (response.usageMetadata?.promptTokenCount || 0) + (response.usageMetadata?.candidatesTokenCount || 0)
            };

            return; // Success, exit loop
        } catch (error: any) {
            console.warn(`Stream Story Model ${currentModel} failed:`, error.message);
            lastError = error;
        }
    }
    throw lastError || new Error("All stream story models failed.");
}

// Logic Model: Gemini 2.5 Flash (Optimized for speed & JSON)
export async function generateGameLogic(
    apiKey: string,
    lastUserMessage: string,
    lastAiResponse: string,
    gameState: any // [CHANGED] Accepts full GameState to generate shared static prompt
) {
    if (!apiKey) throw new Error('API Key is missing');

    console.log(`[GeminiLogic] GENERATE GAME LOGIC CALLED for GameID: ${gameState.activeGameId}`);

    // [REFACTOR] Event System Decoupled to Parallel Agent
    // Events are no longer processed inside the monolithic Logic Model loop.
    // They are handled by AgentOrchestrator -> EventManager in Phase 2.
    // validEvents is kept empty here to minimize token usage for the Logic Model.
    let validEvents: any[] = [];

    const genAI = new GoogleGenerativeAI(apiKey);

    // [CONTEXT CACHING] Reuse the SAME static prefix
    const staticPrompt = await PromptManager.getSharedStaticContext(gameState);

    // Logic uses MODEL_CONFIG.LOGIC (Optimized for speed)
    const modelsToTry = [MODEL_CONFIG.LOGIC, MODEL_CONFIG.PRE_LOGIC];

    for (const modelName of modelsToTry) {
        try {
            console.log(`Trying logic model: ${modelName}`);

            // [NEW] Static Prompt for Context Caching
            // [NEW] Static Prompt for Context Caching
            // [OPTIMIZATION] Convert JSON to YAML-like text using LoreConverter to save tokens & match Story Model format
            const rankCriteria = gameState.lore?.martial_arts_levels ? LoreConverter.convertMartialArtsLevels(gameState.lore.martial_arts_levels) : null;
            const romanceGuide = gameState.lore?.romance_guide ? LoreConverter.convertRomance(gameState.lore.romance_guide) : null;
            const combatGuide = gameState.lore?.combat_guide ? LoreConverter.convertCombat(gameState.lore.combat_guide) : null;

            const staticLogicPrompt = getStaticLogicPrompt(gameState.activeGameId, rankCriteria, romanceGuide, combatGuide);

            // [NOTE] Logic Model is usually too small for Context Caching (<32k)
            // So we skip the getOrUpdateCache step here to save latency. 
            // If Logic contexts ever get huge, use the same pattern as generateResponse.

            const model = genAI.getGenerativeModel({
                model: modelName,
                safetySettings,
                generationConfig: { responseMimeType: "application/json" },
                systemInstruction: staticLogicPrompt // [OPTIMIZATION] Cache the Rules & Format
            });

            // Logic Prompt (Dynamic Suffix)
            // Note: We need to pass pruned stats for logic processing logic, but PromptManager uses full state.
            // Ideally, we prune stats separately.
            const stats = gameState.playerStats || {};
            // Prune huge arrays to save tokens in the Dynamic part (Static part is already big but cached)
            const {
                characterData,
                availableBackgrounds,
                availableCharacterImages,
                availableExtraImages,
                chatHistory, // [Optimize] Remove history from stats dumping (Passed separately as strings)
                displayHistory,
                worldData: _unusedWorldData, // [FIX] Exclude worldData from prunedStats (Renamed to avoid conflict)
                lore, // [CRITICAL FIX] Exclude Lore from Logic Model (It uses huge tokens and isn't needed for logic)
                wikiData, // [OPTIMIZATION] Exclude Wiki Data (Generic info not needed for step logic)
                scriptQueue, // [OPTIMIZATION] Exclude Visual Novel Script Queue
                textMessageHistory, // [OPTIMIZATION] Exclude SMS History
                constants, // [OPTIMIZATION] Exclude Static Constants
                events: _ignoredEvents, // [OPTIMIZATION] Exclude Raw Events List (Passed filtered)
                characterMap,
                extraMap,
                characterCreationQuestions,
                backgroundMappings, // [OPTIMIZATION] Exclude Background Mappings
                ...prunedStats
            } = gameState;

            const worldData = gameState.worldData || { locations: {}, items: {} }; // Fallback to empty if missing

            // Get lightweight context for Logic Model
            const logicContext = PromptManager.getLogicModelContext(gameState);

            // [NEW] Dynamic Prompt (Stats & Action)
            const prompt = getDynamicLogicPrompt(
                prunedStats,
                lastUserMessage,
                lastAiResponse,
                logicContext,
                worldData,
                validEvents // [NEW] Pass filtered events to logic prompt
            );

            // [DEBUG LOG] Verify Logic Model Prompt Size
            console.log("--- [Logic Model Input Prompt] ---");
            console.log(prompt);
            console.log("----------------------------------");

            // [Retry Wrapper] logic generation
            const result = await retryWithBackoff(
                () => model.generateContent(prompt),
                3,
                1500,
                `Logic Generation (${modelName})`
            );
            const response = result.response;
            const text = response.text();

            console.log("Raw Logic Response:", text); // Debug Log

            try {
                const json = JSON.parse(text);
                console.log("Parsed Logic JSON:", json); // Debug Log

                // Attach usage metadata if available
                if (response.usageMetadata) {
                    json._usageMetadata = response.usageMetadata;
                }

                // Attach debug info
                json._debug_prompt = prompt;
                json._debug_static_prompt = staticLogicPrompt;
                json._debug_raw_response = text;

                // [REFACTOR] Event Trigger Logic Moved
                // Logic Model no longer selects events.
                // EventManager (Phase 2 Parallel) handles selection and Next State Injection.
                if (json.triggerEventId) {
                    console.warn(`[GeminiLogic] DEPRECATED: Logic Model returned triggerEventId '${json.triggerEventId}' but Event System is now parallel.`);
                }

                return json;
            } catch (e) {
                console.error("Failed to parse logic JSON:", e);
                // Try to extract JSON if it's wrapped in markdown code blocks
                const match = text.match(/```json([\s\S]*?)```/);
                if (match) {
                    try {
                        const json = JSON.parse(match[1]);
                        console.log("Parsed Logic JSON (from markdown):", json); // Debug Log

                        // Attach debug info
                        json._debug_prompt = prompt;
                        json._debug_static_prompt = staticLogicPrompt;
                        json._debug_raw_response = text;

                        return json;
                    } catch (e2) {
                        console.error("Failed to parse extracted JSON:", e2);
                    }
                }
                // If parsing failed, continue to next model (maybe model output was bad)
                throw new Error("JSON parsing failed");
            }

        } catch (error: any) {
            console.warn(`Logic Model ${modelName} failed:`, error.message || error);
            // Continue to next model
        }
    }

    return null;
}

// [NEW] Casting Logic Model: Gemini 2.5 Flash
export async function generateCastingDecision(
    apiKey: string,
    context: {
        location: string;
        summary: string;
        userInput: string;
        activeCharacters: string[];
    },
    candidates: { id: string; name: string; reasons: string[]; score: number }[],
    bgCandidates: { id: string; name: string; reasons: string[]; score: number }[],
    modelName: string = MODEL_CONFIG.LOGIC
) {
    if (!apiKey) throw new Error('API Key is missing');
    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({
        model: modelName,
        safetySettings,
        generationConfig: { responseMimeType: "application/json" }
    });

    const prompt = `
    ë‹¹ì‹ ì€ ë¹„ì£¼ì–¼ ë…¸ë²¨ ê²Œì„ì˜ 'ìºìŠ¤íŒ… ë””ë ‰í„°'ì…ë‹ˆë‹¤.
    í˜„ì¬ ì¥ë©´ì˜ ê¸´ì¥ê°, ì¬ë¯¸, ì„œì‚¬ì  ê°œì—°ì„±ì„ ê³ ë ¤í•˜ì—¬ **ê°€ì¥ ì ì ˆí•œ ë“±ì¥ì¸ë¬¼**ì„ ì„ ë³„í•´ì•¼ í•©ë‹ˆë‹¤.

    [í˜„ì¬ ìƒí™©]
    - ì¥ì†Œ: ${context.location}
    - ì´ì „ ì¤„ê±°ë¦¬: ${context.summary}
    - í”Œë ˆì´ì–´ í–‰ë™: "${context.userInput}"
    - í˜„ì¬ í™”ë©´ì— ìˆëŠ” ì¸ë¬¼: ${JSON.stringify(context.activeCharacters)}

    [í›„ë³´ ëª…ë‹¨ (ì ìˆ˜ìˆœ ì •ë ¬ë¨)]
    ${JSON.stringify(candidates.map(c => ({ id: c.id, name: c.name, score: c.score.toFixed(1), reasons: c.reasons })))}
    
    [ë°°ê²½ ì¸ë¬¼ í›„ë³´ (ì ìˆ˜ìˆœ)]
    ${JSON.stringify(bgCandidates.map(c => ({ id: c.id, name: c.name, score: c.score.toFixed(1), reasons: c.reasons })))}

    [ì§€ì‹œì‚¬í•­]
    1. **Active ì„ ë°œ (ìµœëŒ€ 6ëª…)**: ëŒ€í™”ë‚˜ ì‚¬ê±´ì˜ ì£¼ì²´ê°€ ë  ì¸ë¬¼ì„ ë½‘ìœ¼ì„¸ìš”.
       - ì ìˆ˜ê°€ ë†’ë”ë¼ë„, ì§€ê¸ˆ ë¶„ìœ„ê¸°ì— ë§ì§€ ì•Šê±°ë‚˜ ë„ˆë¬´ ëœ¬ê¸ˆì—†ìœ¼ë©´ ì œì™¸í•˜ì„¸ìš”.
       - ë°˜ëŒ€ë¡œ ì ìˆ˜ê°€ ë‚®ë”ë¼ë„, **"ì§€ê¸ˆ ë“±ì¥í•˜ë©´ ê°ˆë“±ì´ ê³ ì¡°ë˜ê±°ë‚˜ ì¬ë¯¸ìˆê² ë‹¤"** ì‹¶ì€ ì¸ë¬¼ì€ ê³¼ê°íˆ ë°œíƒí•˜ì„¸ìš”.
       - ì£¼ì¸ê³µì´ ì§ì ‘ ë¶€ë¥¸ ì¸ë¬¼ì€ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
       - **ì¤‘ìš”**: ê° Active ì¸ë¬¼ì— ëŒ€í•´ "ì™œ ì§€ê¸ˆ ë“±ì¥í•˜ë©°, ë¬´ì—‡ì„ í•˜ëŠ”ì§€"ì— ëŒ€í•œ ì§§ì€ **[ë“±ì¥ ì‹œë‚˜ë¦¬ì˜¤]**ë¥¼ ì œì•ˆí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨ìˆœ ë“±ì¥ì´ ì•„ë‹ˆë¼, ì‚¬ê±´ì´ë‚˜ ëŒ€í™”ë¥¼ ìœ ë°œí•˜ëŠ” êµ¬ì²´ì ì¸ í–‰ë™ì„ ì ìœ¼ì„¸ìš”.
    2. **Background ì„ ë°œ (ìµœëŒ€ 12ëª…)**: ë°°ê²½ì— ì„œ ìˆì„ ì¸ë¬¼ì„ ë½‘ìœ¼ì„¸ìš”.
       - ì¥ì†Œì— ì–´ìš¸ë¦¬ëŠ” ì¸ë¬¼(ì£¼ì¸, ì§ì› ë“±) ìš°ì„ .
       - Active ì¸ë¬¼ì˜ ì§€ì¸/ë¶€í•˜ ë“±.

    [Output Format (JSON)]
    {
        "active": [
            { "id": "char_id_1", "scenario": "Brief description of arrival action (e.g. 'Suddenly bursts in kicking the door')." },
            { "id": "char_id_2", "scenario": "Quietly observing from corner." }
        ],
        "background": ["id3", "id4", "id5"],
        "reason": "Brief explanation of why this casting was chosen."
    }
    `;

    try {
        const result = await retryWithBackoff(
            () => model.generateContent(prompt),
            2,
            1000,
            `Casting Decision (${modelName})`
        );
        const text = result.response.text();
        console.log(`[GeminiCasting] Raw AI Decision: ${text.substring(0, 100)}...`);
        return JSON.parse(text);
    } catch (e: any) {
        console.warn("AI Casting Failed:", e.message);
        return null; // Fallback to heuristic
    }
}

// Memory Summarization Model: Gemini 1.5 Flash (Cost-efficient)
export async function generateSummary(
    apiKey: string,
    currentSummary: string,
    recentDialogue: Message[]
) {
    if (!apiKey) return currentSummary;

    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({
        model: MODEL_CONFIG.SUMMARY,
        safetySettings
    });

    const dialogueText = recentDialogue.map(msg =>
        `${msg.role === 'user' ? 'Player' : 'AI'}: ${msg.text}`
    ).join('\n');

    const prompt = `
    ë‹¹ì‹ ì€ ë°©ëŒ€í•œ ì„œì‚¬ë¥¼ í•µì‹¬ë§Œ ì¶”ë ¤ ê¸°ë¡í•˜ëŠ” 'ì™•ì‹¤ ì„œê¸°'ì…ë‹ˆë‹¤.
    í”Œë ˆì´ì–´ì˜ ëª¨í—˜ì´ ê¸¸ì–´ì§ì— ë”°ë¼, ì˜¤ë˜ëœ ê¸°ë¡ì€ ê³¼ê°íˆ ì¶•ì•½í•˜ê³  **í˜„ì¬ì˜ ìƒíƒœì™€ ì§ë©´í•œ ìƒí™©** ìœ„ì£¼ë¡œ ìš”ì•½ë³¸ì„ ê°±ì‹ í•´ì•¼ í•©ë‹ˆë‹¤.

    [ì…ë ¥ ë°ì´í„°]
    1. ì´ì „ ìš”ì•½ (Save Data): ${currentSummary || "ì—†ìŒ"}
    2. ìµœê·¼ ëŒ€í™” (Log): 
    ${dialogueText}

    [ì‘ì„± ì§€ì¹¨]
    1. **ì••ì¶• ë° ê°±ì‹ **: ê³¼ê±°ì˜ 'ì´ì „ ìš”ì•½'ê³¼ 'ìµœê·¼ ëŒ€í™”'ë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ ìƒˆë¡œìš´ ìš”ì•½ë¬¸ì„ ë§Œë“œì‹­ì‹œì˜¤. ë‹¨ìˆœí•œ ë§ë¶™ì´ê¸°ê°€ ì•„ë‹™ë‹ˆë‹¤.
    2. **ë¶„ëŸ‰ ì œí•œ**: ì „ì²´ ìš”ì•½ì€ ì ˆëŒ€ *2500ì(ê³µë°± í¬í•¨)** ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ë¬¸ì¥ì„ ê°„ê²°í•˜ê²Œ ë‹¤ë“¬ìœ¼ì‹­ì‹œì˜¤. ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ê°€ì¥ ì˜¤ë˜ëœ ì‚¬ê±´ë¶€í„° ê³¼ê°íˆ ì‚­ì œí•˜ê±°ë‚˜ í•œ ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•˜ì‹­ì‹œì˜¤.
    3. **í˜„ì¬ì„± ìœ ì§€**: ì§€ê¸ˆ í”Œë ˆì´ì–´ê°€ ì–´ë””ì— ìˆê³ , ë¬´ì—‡ì„ í•˜ê³ , ëˆ„êµ¬ì™€ í•¨ê»˜ ìˆëŠ”ì§€, ì–´ë–¤ ë¶€ìƒì„ ì…ì—ˆëŠ”ì§€ **'í˜„ì¬ ìƒíƒœ'**ê°€ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.
    4. **í˜•ì‹**: ì¤„ê¸€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ê°€ë…ì„±ì„ ìœ„í•´ ë‹¨ë½ì„ ë‚˜ëˆ„ì‹­ì‹œì˜¤.

    [í•„ìˆ˜ í¬í•¨ í•­ëª©]
    - í˜„ì¬ ìœ„ì¹˜ ë° ìƒí™©
    - ì£¼ìš” ì¸ë¬¼ê³¼ì˜ ê´€ê³„ ë³€í™” (íŠ¹íˆ í˜¸ê°ë„/ì ëŒ€ê°)
    - í˜„ì¬ íšë“í•œ ë¹„ê¸‰/ì•„ì´í…œ ë° ì‹ ì²´ ìƒíƒœ(ë¶€ìƒ ë“±)
    - ë‹¹ë©´í•œ ëª©í‘œ

    [ê°±ì‹ ëœ ìš”ì•½ë³¸]:
    `;

    try {
        const result = await model.generateContent(prompt);
        const response = result.response;
        const text = response.text();
        console.log("Generated Summary:", text);
        return text;
    } catch (error: any) {
        console.warn("Summarization failed:", error.message || error);
        return currentSummary; // Return old summary on failure
    }
}

// Memory Summarization for Characters: Use Configured Model (Gemini 2.5 Flash)
export async function generateCharacterMemorySummary(
    apiKey: string,
    characterName: string,
    existingMemories: string[]
): Promise<string[]> {
    if (!apiKey || existingMemories.length <= 10) return existingMemories;

    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({
        model: MODEL_CONFIG.SUMMARY, // Logic/Flash Model
        safetySettings
    });

    const memoryText = existingMemories.map(m => `- ${m}`).join('\n');

    const prompt = `
    ë‹¹ì‹ ì€ RPG ê²Œì„ì˜ 'ìºë¦­í„° ê¸°ì–µ ê´€ë¦¬ì'ì…ë‹ˆë‹¤.
    í˜„ì¬ [${characterName}] ìºë¦­í„°ì˜ ê¸°ì–µì´ ë„ˆë¬´ ë§ì•„ì ¸ì„œ ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    ìì˜í•œ ê¸°ì–µë“¤ì„ ë³‘í•©í•˜ê³ , ì¤‘ìš”í•œ ì‚¬ê±´ ìœ„ì£¼ë¡œ ìš”ì•½í•˜ì—¬ **10ê°œ ì´í•˜ì˜ í•µì‹¬ ê¸°ì–µ ë¦¬ìŠ¤íŠ¸**ë¡œ ì¬êµ¬ì„±í•˜ì‹­ì‹œì˜¤.

    [ì…ë ¥ëœ ê¸°ì–µ ëª©ë¡]
    ${memoryText}

    [ì‘ì„± ì§€ì¹¨]
    1. **ì¤‘ìš”ë„ ê¸°ë°˜ ì„ ë³„**: ì•½ì†, ì›í•œ, ìƒëª…ì˜ ì€ì¸, ì£¼ìš” í€˜ìŠ¤íŠ¸ ê´€ë ¨ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë‚¨ê¸°ì‹­ì‹œì˜¤.
    2. **ë³‘í•©**: "ê°™ì´ ë°¥ì„ ë¨¹ìŒ", "ê°™ì´ ì°¨ë¥¼ ë§ˆì‹¬" -> "í”Œë ˆì´ì–´ì™€ ì‹ì‚¬ë¥¼ í•˜ë©° ì¹œí•´ì§" ì²˜ëŸ¼ ë¹„ìŠ·í•œ ì‚¬ê±´ì€ í•˜ë‚˜ë¡œ í•©ì¹˜ì‹­ì‹œì˜¤.
    3. **ìµœì‹ ì„± ìœ ì§€**: ê°€ì¥ ìµœê·¼ì— ë°œìƒí•œ ì¤‘ìš”í•œ ë³€í™”ëŠ” ìƒì„¸íˆ ê¸°ë¡í•˜ì‹­ì‹œì˜¤.
    4. **í˜•ì‹**: JSON String Array ("[...]").

    [ìš”ì•½ëœ ê¸°ì–µ ë¦¬ìŠ¤íŠ¸ (JSON Array)]:
    `;

    try {
        const result = await model.generateContent(prompt);
        const response = result.response;
        const text = response.text();

        // JSON Parsing Attempt
        const firstBracket = text.indexOf('[');
        const lastBracket = text.lastIndexOf(']');

        if (firstBracket !== -1 && lastBracket !== -1) {
            const jsonText = text.substring(firstBracket, lastBracket + 1);
            const parsed = JSON.parse(jsonText);
            if (Array.isArray(parsed)) {
                console.log(`[MemorySummary] Summarized ${characterName}'s memories: ${existingMemories.length} -> ${parsed.length}`);
                return parsed;
            }
        }

        throw new Error("Failed to parse JSON response");

    } catch (error: any) {
        console.warn(`[MemorySummary] Failed to summarize memories for ${characterName}:`, error);
        // Fallback: Just keep the last 10 if AI fails
        return existingMemories.slice(-10);
    }
}

// [Startup Warmup]
// Fire-and-forget request to create/warm the cache.
export async function preloadCache(apiKey: string, initialState: any) {
    if (!apiKey) return;
    try {
        console.log("[Gemini] Pre-loading cache...");

        // 1. Get the Exact Same Static Prompt as normal requests
        const staticPrompt = await PromptManager.getSharedStaticContext(initialState, initialState.activeChars, initialState.spawnCandidates);

        // [CACHE WARMUP FIX]
        // Instead of dummy generating, we should try to create the cache object itself here if possible.
        // But for consistency, let's just trigger the normal flow or call getOrUpdateCache directly.
        // We need 'modelName' which is MODEL_CONFIG.STORY.

        // This won't actually "warm" the model inferencing, but it might create the cache entry.
        const modelName = MODEL_CONFIG.STORY;
        const overrideKey = initialState.personaOverride ? `_PERSONA_${initialState.personaOverride}` : '';
        const cacheDisplayName = `CACHE_${initialState.activeGameId}_STORY_v1.3${overrideKey}`;

        if (staticPrompt.length > 50000) {
            await getOrUpdateCache(apiKey, cacheDisplayName, staticPrompt, modelName);
            console.log("[Gemini] Cache Entry Verified/Created during warmup.");
        }

        // 2. Configure Model
        // MATCH THE MAIN MODEL: MODEL_CONFIG.STORY
        const genAI = new GoogleGenerativeAI(apiKey);
        const model = genAI.getGenerativeModel({
            model: MODEL_CONFIG.STORY,
            systemInstruction: staticPrompt,
        }, {
            apiVersion: 'v1beta',
        });

        // 3. Send a dummy prompt
        const result = await model.generateContent("System Initialization (Warmup)");

        console.log("[Gemini] Cache Warmup Request Sent!");
        return result.response.usageMetadata;
    } catch (e) {
        console.error("[Gemini] Cache Warmup Failed:", e);
        return null;
    }
}

// =========================================================
// Turn Orchestration Logic
// =========================================================

const SUMMARY_THRESHOLD = 3;

export async function handleGameTurn(
    apiKey: string,
    state: any,             // State including turnCount and summary
    history: Message[],     // UI display history
    userInput: string
) {
    console.log(`[handleGameTurn] Turn: ${state.turnCount}, Summary Length: ${state.scenarioSummary?.length || 0
        }`);

    // 1. Add user message to a temporary history for the AI call
    // Note: The caller updates the actual UI history. This is for the AI logic.
    // If we want to strictly follow the user's snippet, we operate on the list passed.
    const newHistory = [...history, { role: 'user', text: userInput } as Message];

    // 2. Main Story Model
    // State should already have the summary injected via PromptManager in generateResponse
    const storyResult = await generateResponse(
        apiKey,
        history,
        userInput,
        state,
        'ko' // Default to Korean as per context
    );

    // 3. Game Logic Model
    const logicResult = await generateGameLogic(
        apiKey,
        userInput,
        storyResult.text,
        state
    );

    // Merge logic result into nextState (Conceptually)
    // The actual state update depends on the caller (client or server action) applying the JSON.
    // But for the purpose of summarization, we need to know the *next* turn count.

    let nextState = { ...state, ...logicResult };

    // [New] Process Time & Survival Logic
    const currentStats = nextState.playerStats || {};
    let currentFatigue = currentStats.fatigue || 0;

    // 1. Fatigue Update from Logic
    if (logicResult.fatigueChange) {
        currentFatigue = Math.max(0, Math.min(100, currentFatigue + logicResult.fatigueChange));
    }

    // 2. Time & Sleep Logic
    let currentDay = nextState.day || 1;
    let currentTime = nextState.time || 'Morning';
    const timePhases = ['Morning', 'Afternoon', 'Evening', 'Night'];

    if (logicResult.isSleep) {
        // Sleep Action
        if (currentTime === 'Night') {
            // Full Rest: Next Day
            currentDay++;
            currentTime = 'Morning';
            currentFatigue = 0; // Reset Fatigue
        } else {
            // Nap: Reduce Fatigue, Advance Time (1 phase)
            currentFatigue = Math.max(0, currentFatigue - 30);
            const nextIdx = (timePhases.indexOf(currentTime) + 1) % 4;
            if (nextIdx === 0) currentDay++;
            currentTime = timePhases[nextIdx];
        }
    } else {
        // Normal Time Progression (Support both new 'timeConsumed' and legacy 'timeProgress')
        const timeAdvance = logicResult.timeConsumed || (logicResult.timeProgress ? 1 : 0);

        if (timeAdvance > 0) {
            let idx = timePhases.indexOf(currentTime);

            // Advance by N steps
            for (let i = 0; i < timeAdvance; i++) {
                idx++;
                if (idx >= timePhases.length) {
                    idx = 0;
                    currentDay++; // Night -> Morning triggers next day
                }
            }
            currentTime = timePhases[idx];
        }
    }

    // Apply Time/Fatigue Updates to State
    nextState.day = currentDay;
    nextState.time = currentTime;
    nextState.playerStats = { ...currentStats, fatigue: currentFatigue };

    // Logic result might have specific fields like hpChange, etc. 
    // We are interested in 'turnCount' and 'scenarioSummary'.
    // The user's snippet implies we handle turn counting here or assume it's passed.
    // If the state passed in is the *current* state (before this turn), we should increments turn info.

    const currentTurnCount = (state.turnCount || 0) + 1;

    // Update summary if threshold reached
    let newSummary = state.scenarioSummary;

    if (currentTurnCount > 0 && currentTurnCount % SUMMARY_THRESHOLD === 0) {
        console.log(`[handleGameTurn] Triggering Memory Summarization at turn ${currentTurnCount}...`);

        // A. Select recent dialogue (Last 10 turns = 20 messages)
        // We use newHistory which includes the latest user message.
        // We also need the model's response we just generated.
        const fullDialogue = [...newHistory, { role: 'model', text: storyResult.text } as Message];
        const recentDialogue = fullDialogue.slice(-SUMMARY_THRESHOLD * 2);

        // B. Generate Summary
        newSummary = await generateSummary(
            apiKey,
            state.scenarioSummary || "",
            recentDialogue
        );

        console.log("[handleGameTurn] New Summary Generated.");
    }

    // Calculate Total Cost
    let totalCost = 0;

    // 1. Story Cost
    if (storyResult.usageMetadata) {
        // [FIX] We need to report the cached tokens to the calculator if they existed
        // Our 'storyResult.totalTokenCount' is a sum, but cost needs specific breakdown.
        // UsageMetadata has 'cachedContentTokenCount' (if supported by backend).
        // Let's ensure we account for it.
        const meta = storyResult.usageMetadata;
        totalCost += calculateCost(
            storyResult.usedModel || MODEL_CONFIG.STORY,
            meta.promptTokenCount || 0,
            meta.candidatesTokenCount || 0,
            meta.cachedContentTokenCount || 0
        );
    }

    // 2. Logic Cost (Assuming LOGIC model)
    if (logicResult._usageMetadata) {
        totalCost += calculateCost(
            MODEL_CONFIG.LOGIC,
            logicResult._usageMetadata.promptTokenCount || 0,
            logicResult._usageMetadata.candidatesTokenCount || 0
        );
    }

    // 3. Summary Cost (Assuming SUMMARY model, if generated)
    // Note: Summary generation doesn't currently return metadata easily, 
    // but it's small. We can ignore or approximate if needed.
    // For perfection, we'd need generateSummary to return metadata.
    // Let's assume 0 for optional summary for now or update generateSummary later.

    return {
        reply: storyResult.text,
        logic: logicResult,
        summary: newSummary,
        turnCount: currentTurnCount,
        storyResult: storyResult,
        cost: totalCost // [New]
    };
}
