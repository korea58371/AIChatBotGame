import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from '@google/generative-ai';

import { Message } from '../store';
import { PromptManager } from '../engine/prompt-manager';
import { LoreConverter } from '../utils/lore-converter';
import { getLogicPrompt, getStaticLogicPrompt, getDynamicLogicPrompt } from '@/data/prompts/logic';
import { MODEL_CONFIG, calculateCost } from './model-config';
import { EventManager } from '../engine/event-manager';

// Removed static SYSTEM_PROMPT in favor of dynamic generation

const safetySettings = [
    {
        category: HarmCategory.HARM_CATEGORY_HARASSMENT,
        threshold: HarmBlockThreshold.BLOCK_NONE,
    },
    {
        category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold: HarmBlockThreshold.BLOCK_NONE,
    },
    {
        category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold: HarmBlockThreshold.BLOCK_NONE,
    },
    {
        category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold: HarmBlockThreshold.BLOCK_NONE,
    },
];

// Helper: Retry with Exponential Backoff
export async function retryWithBackoff<T>(
    fn: () => Promise<T>,
    retries: number = 3,
    delay: number = 1000,
    operationName: string = 'Gemini API'
): Promise<T> {
    try {
        return await fn();
    } catch (error: any) {
        if (retries === 0 || !shouldRetry(error)) throw error;

        console.warn(`[Gemini] ${operationName} failed. Retrying... (${retries} attempts left). Error: ${error.message}`);
        await new Promise(resolve => setTimeout(resolve, delay));
        return retryWithBackoff(fn, retries - 1, delay * 2, operationName);
    }
}

function shouldRetry(error: any): boolean {
    const msg = (error.message || '').toLowerCase();
    // 500: Internal Server Error
    // 503: Service Unavailable
    // 429: Too Many Requests (Rate Limit) -> Wait longer? Default backoff helps.
    // fetch failed: Network issues
    return msg.includes('500') || msg.includes('503') || msg.includes('429') || msg.includes('internal error') || msg.includes('fetch failed') || msg.includes('overloaded');
}

// [NEW] Helper: Get or Create Cache
// Gemini requires minimal 32k tokens for caching. Standard logic prompts are too small (~5k).
// Only Story Prompt (~35k) validates for caching.
// [Refactor] Dynamic Import Wrapper
async function getOrUpdateCache(
    apiKey: string,
    cacheKey: string,
    systemInstruction: string,
    modelName: string
): Promise<string | null> {
    // 1. Environment Check
    if (typeof window !== 'undefined') {
        console.warn("[GeminiCache] Attempted to access CacheManager from Client Side. Skipping.");
        return null; // Client side cannot cache
    }

    try {
        // 2. Dynamic Import of Server-Only Module
        // @ts-ignore - Build time check bypass
        const { getOrUpdateCache: serverGetOrUpdateCache } = await import('./gemini-server');
        return await serverGetOrUpdateCache(apiKey, cacheKey, systemInstruction, modelName);

    } catch (error) {
        console.error("[GeminiCache] Failed to load server module:", error);
        return null;
    }
}

export async function generateResponse(
    apiKey: string,
    history: Message[],
    userMessage: string,
    gameState: any, // Pass the full game state to generate the prompt
    language: 'ko' | 'en' | null,
    modelName: string = MODEL_CONFIG.STORY // [NEW] Accept model override, default to config
) {
    if (!apiKey) throw new Error('API Key is missing');

    console.log(`[Gemini] generateResponse called. Model: ${modelName}. gameState.playerStats.luk: ${gameState.playerStats?.luk}`);

    const genAI = new GoogleGenerativeAI(apiKey);

    // Generate dynamic system prompt based on current game state

    // [SERVER REHYDRATION FIX]
    // If getSystemPromptTemplate is missing (because functions don't survive network serialization),
    // and we have activeGameId, we try to reload it dynamically.
    if (!gameState.getSystemPromptTemplate && gameState.activeGameId) {
        console.log(`[Gemini] Rehydrating System Prompt for game: ${gameState.activeGameId}`);
        try {
            // Dynamic import based on game ID
            const systemModule = await import(`@/data/games/${gameState.activeGameId}/prompts/system`);
            if (systemModule && systemModule.getSystemPromptTemplate) {
                gameState.getSystemPromptTemplate = systemModule.getSystemPromptTemplate;
            }
        } catch (e) {
            console.warn(`[Gemini] Failed to rehydrate system prompt for ${gameState.activeGameId}:`, e);
        }
    }

    const staticPrompt = await PromptManager.getSharedStaticContext(gameState);
    const dynamicPrompt = PromptManager.generateSystemPrompt(gameState, language, userMessage);

    // [DEBUG] Check Lore Injection
    if (gameState.lore) {
        // console.log(`[Gemini] Lore Data Present. Keys: ${Object.keys(gameState.lore).join(', ')}`);
    } else {
        console.warn("[Gemini] âš ï¸ Lore Data is MISSING in gameState!");
    }

    // [ì»¨í…ìŠ¤íŠ¸ ìºì‹± í‚¤ ìˆ˜ì •]
    // ì •ì (Static) í”„ë¡¬í”„íŠ¸ì™€ ë™ì (Dynamic) í”„ë¡¬í”„íŠ¸ë¥¼ í•©ì¹˜ì§€ ë§ˆì„¸ìš”.
    // Static = systemInstruction (ìºì‹œë¨, ë¹„ìš© ì ˆê°)
    // Dynamic = ìœ ì € ë©”ì‹œì§€ì˜ ì¼ë¶€ (ìºì‹œ ì•ˆë¨, ë§¤ë²ˆ ë³€ê²½)
    const systemInstruction = staticPrompt;

    // [Fix] Use passed modelName if available, otherwise default to config
    // Also ensuring no variable shadowing in loop
    const targetModel = modelName || MODEL_CONFIG.STORY;

    // [CACHE IMPLEMENTATION - STORY MODEL ONLY]
    // We try to get a valid cache name.
    let cachedContentName: string | null = null;

    // Determine the Cache Key: Same logic as PromptManager but cleaner
    const overrideKey = gameState.personaOverride ? `_PERSONA_${gameState.personaOverride}` : '';
    // Use a simpler display name that matches PromptManager's logic but safer for display name (no spaces if possible, though Gemini allows)
    // PromptManager Key: PROMPT_CACHE_{GameID}_SHARED_{Version}
    // We reuse PromptManager's public method logic if possible, or replicate it.
    // Let's replicate the key structure: `CACHE_{GameID}_STORY_{Version}`
    // [Fix] Cache Mismatch Error (400): Model used by GenerateContent (Flash) and CachedContent (Pro) must be same.
    // Solution: Include Model Name in Key.
    const sanitizedModelName = targetModel.replace(/[^a-zA-Z0-9]/g, '_').toUpperCase();
    const cacheDisplayName = `CACHE_${gameState.activeGameId}_STORY_v2_0_${sanitizedModelName}${overrideKey}`;

    // Only attempt caching if the prompt is substantial (Static Prompt is huge)
    // And only for Story models (Logic is too small)
    console.log(`[GeminiCache] Static Prompt Length: ${systemInstruction.length} chars.`);

    if (systemInstruction.length > 25000) { // Lowered to 30k chars as safety margin for 32k tokens (which is roughly ~100k chars? No, tokens > chars/4 usually? Korean is distinct.)
        // Actually 1 token ~= 1.5-3 chars for Korean? 
        // 32k tokens * 2 chars = 64k chars. 
        // If 34k tokens total input, and static is 90% of it, it should be huge.
        // Let's rely on the server to reject if too small.
        console.log(`[GeminiCache] Attempting to retrieve/create cache for key: ${cacheDisplayName}`);
        cachedContentName = await getOrUpdateCache(apiKey, cacheDisplayName, systemInstruction, targetModel);
        console.log(`[GeminiCache] getOrUpdateCache Result: ${cachedContentName}`);
    } else {
        console.log(`[GeminiCache] Static Prompt length (${systemInstruction.length} chars) below threshold (30000). Skipping Cache.`);
    }

    const modelsToTry = [
        targetModel,
        MODEL_CONFIG.LOGIC, // Fallback
    ];

    let lastError;

    // Rename loop variable to avoid shadowing arg 'modelName'
    for (const currentModel of modelsToTry) {
        try {
            console.log(`Trying story model: ${currentModel}`);

            const modelConfig: any = {
                model: currentModel,
                safetySettings
            };

            // [Gemini 3 ìµœì í™”] Native Thinking í™œì„±í™”
            // [Fix] Flash ëª¨ë¸ì€ ì†ë„ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ Thinkingì„ ë¹„í™œì„±í™”í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
            // ëª…ì‹œì ì¸ 'thinking' ëª¨ë¸ì´ê±°ë‚˜, 'gemini-3' ì´ë©´ì„œ 'flash'ê°€ ì•„ë‹Œ ê²½ìš°(Pro ë“±)ì—ë§Œ í™œì„±í™”
            if (currentModel.includes('thinking') || (currentModel.includes('gemini-3') && !currentModel.includes('flash'))) {
                modelConfig.thinkingConfig = {
                    includeThoughts: true, // [User Request] ìƒê° ê³¼ì • ë¡œê·¸ í™•ì¸ì„ ìœ„í•´ True ì„¤ì •
                    thinkingLevel: "high"
                };
            }

            // [CRITICAL] CACHE INJECTION
            // If we have a valid cache, we supply 'cachedContent' and REMOVE 'systemInstruction'.
            // The SDK pattern for cached content:
            if (cachedContentName && currentModel === targetModel) { // Only use cache for the target model it was created for
                console.log(`[Gemini] USING CACHED CONTENT: ${cachedContentName}`);
                modelConfig.cachedContent = { name: cachedContentName };
                // systemInstruction MUST BE REMOVED if cachedContent is present? 
                // Actually, the cache *contains* the systemInstruction. 
                // We should NOT pass it again as 'systemInstruction' property if it's in cache.
            } else {
                console.log(`[Gemini] Using Standard System Instruction (No Cache)`);
                modelConfig.systemInstruction = systemInstruction;
            }

            // [DYNAMIC MODEL] Override model if provided (already handled by logic above, ensuring key matches)
            // But if we are falling back to LOGIC model in the loop, we shouldn't use the Story Cache (model mismatch)
            if (currentModel !== targetModel && modelConfig.cachedContent) {
                delete modelConfig.cachedContent;
                modelConfig.systemInstruction = systemInstruction; // Restore system prompt for fallback
                modelConfig.model = currentModel;
            }


            const model = genAI.getGenerativeModel(modelConfig);

            // [ìµœì í™”]
            // ìš”ì•½ë³¸ì´ ì¡´ì¬í•œë‹¤ë©´ ì „ì²´ íˆìŠ¤í† ë¦¬ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
            // ìµœê·¼ ëŒ€í™”(ì˜ˆ: ë§ˆì§€ë§‰ 10ê°œ ë©”ì‹œì§€ = 5í„´)ë§Œ ë‚¨ê²¨ í† í°ì„ ì ˆì•½í•˜ê³ 
            // ìš”ì•½ë³¸ê³¼ ì „ì²´ íˆìŠ¤í† ë¦¬ê°€ ì¤‘ë³µë˜ì–´ ì •ë³´ê°€ ì™œê³¡ë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
            let effectiveHistory = history;
            const scenarioMem = gameState.scenarioMemory;
            if (scenarioMem && (scenarioMem.tier1Summaries?.length > 0 || scenarioMem.tier2Summaries?.length > 0)) {
                // ë§ˆì§€ë§‰ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
                effectiveHistory = history.slice(-10);
            }

            // Gemini APIë¥¼ ìœ„í•´ íˆìŠ¤í† ë¦¬ í¬ë§·íŒ… (ë°˜ë“œì‹œ 'user'ë¡œ ì‹œì‘í•´ì•¼ í•¨)
            let processedHistory = effectiveHistory.map(msg => ({
                role: msg.role,
                parts: [{ text: msg.text || "..." }],
            }));

            // ìˆ˜ì • 1: í”„ë¡œí† ì½œì€ 'user'ì™€ 'model'ì´ ë°˜ë“œì‹œ ë²ˆê°ˆì•„ ë‚˜ì™€ì•¼ í•˜ë©°,ì‘ì€ 'user'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
            // ë§Œì•½ íˆìŠ¤í† ë¦¬ê°€ 'model'ë¡œ ì‹œì‘í•œë‹¤ë©´, ë”ë¯¸ 'user' ë©”ì‹œì§€ë¥¼ ì•ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            if (processedHistory.length > 0 && processedHistory[0].role === 'model') {
                processedHistory = [
                    { role: 'user', parts: [{ text: "..." }] }, // ë”ë¯¸ ì»¨í…ìŠ¤íŠ¸ ì‹œì‘
                    ...processedHistory
                ];
            }

            const chatSession = model.startChat({
                history: processedHistory,
            });

            // [ìˆ˜ì •] ë™ì  í”„ë¡¬í”„íŠ¸(ê¸°ë¶„, ìƒíƒœì°½ ë“±)ë¥¼ ìœ ì € ë©”ì‹œì§€ ë‚´ë¶€ì— ì£¼ì…í•©ë‹ˆë‹¤.
            // ì´ë ‡ê²Œ í•˜ë©´ ì •ì  ìºì‹œ(Static Cache)ë¥¼ ê¹¨ì§€ ì•Šìœ¼ë©´ì„œë„ ë§¤ í„´ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            const finalUserMessage = `${dynamicPrompt}\n\n${userMessage}`;

            // [Retry Wrapper]
            const result = await retryWithBackoff(
                () => chatSession.sendMessage(finalUserMessage),
                3,
                1500,
                `Story Generation (${currentModel})`
            );
            const response = result.response;

            // [DEBUG] Log Thinking Process
            // Gemini 3.0 Thinking Model returns thoughts in candidate parts
            const parts = response.candidates?.[0]?.content?.parts || [];
            // Note: The structure might vary, but usually thoughts are tagged or separate.
            // We just log all parts that are NOT the final text if possible, or log everything for inspection.
            // Standard 'text()' method filters out thoughts usually? Let's verify by logging.
            console.log("--- [Gemini Thinking Process] ---");
            parts.forEach((p: any, idx: number) => {
                // Check for thought property (might be specific to SDK version) or just log content
                // If p.thought is true, or check text content
                if (p.thought || (idx === 0 && parts.length > 1)) {
                    console.log(`Thought Part ${idx}:`, p.text || p);
                }
            });
            console.log("---------------------------------");

            return {
                text: response.text(),
                usageMetadata: response.usageMetadata,
                systemPrompt: systemInstruction, // "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"ë¡œ ì •ì  ë¶€ë¶„ ë¡œê·¸
                finalUserMessage: finalUserMessage, // [DEBUG] Return Actual Dynamic Input
                usedModel: currentModel,
                totalTokenCount: (response.usageMetadata?.promptTokenCount || 0) + (response.usageMetadata?.candidatesTokenCount || 0)
            };
        } catch (error: any) {
            console.error(`Story Model ${currentModel} failed:`, error.message || error);
            lastError = error;
            // Continue to next model in list
        }
    }

    throw lastError || new Error("All story models failed.");
}

// [STREAMING IMPLEMENTATION]
// Identical logic to generateResponse, but uses sendMessageStream and returns a generator
export async function* generateResponseStream(
    apiKey: string,
    history: Message[],
    userMessage: string,
    gameState: any,
    language: 'ko' | 'en' | null,
    modelName: string = MODEL_CONFIG.STORY
): AsyncGenerator<string | { usageMetadata: any, systemPrompt: string, finalUserMessage: string, usedModel: string, totalTokenCount: number }, void, unknown> {
    if (!apiKey) throw new Error('API Key is missing');

    console.log(`[GeminiStream] generateResponseStream called. Model: ${modelName}`);

    const genAI = new GoogleGenerativeAI(apiKey);

    // [Rehydration Check]
    if (!gameState.getSystemPromptTemplate && gameState.activeGameId) {
        try {
            const systemModule = await import(`@/data/games/${gameState.activeGameId}/prompts/system`);
            if (systemModule && systemModule.getSystemPromptTemplate) {
                gameState.getSystemPromptTemplate = systemModule.getSystemPromptTemplate;
            }
        } catch (e) { console.warn("Stream Rehydration failed", e); }
    }

    const staticPrompt = await PromptManager.getSharedStaticContext(gameState);
    const dynamicPrompt = PromptManager.generateSystemPrompt(gameState, language, userMessage);
    const systemInstruction = staticPrompt;
    const targetModel = modelName || MODEL_CONFIG.STORY;

    // [Cache Logic]
    let cachedContentName: string | null = null;
    const overrideKey = gameState.personaOverride ? `_PERSONA_${gameState.personaOverride}` : '';
    // [Fix] Cache Mismatch Error (400): Include Model Name in Key
    const sanitizedModelName = targetModel.replace(/[^a-zA-Z0-9]/g, '_').toUpperCase();
    const cacheDisplayName = `CACHE_${gameState.activeGameId}_STORY_v2_0_${sanitizedModelName}${overrideKey}`;

    // [DEBUG: CACHE DISABLED] Threshold raised to 35000 to force NO CACHE (Reverting to "Yesterday" state)
    const cacheStartTime = Date.now();
    if (systemInstruction.length > 25000) {
        cachedContentName = await getOrUpdateCache(apiKey, cacheDisplayName, systemInstruction, targetModel);
    }
    const cacheDuration = Date.now() - cacheStartTime;
    console.log(`[GeminiStreamâ±ï¸] Cache lookup: ${cacheDuration}ms (result: ${cachedContentName ? 'HIT' : 'MISS/SKIP'})`);

    const modelsToTry = [targetModel, MODEL_CONFIG.LOGIC];
    let lastError;

    for (const currentModel of modelsToTry) {
        try {
            console.log(`Trying stream story model: ${currentModel}`);
            const modelConfig: any = { model: currentModel, safetySettings };

            if (currentModel.includes('thinking') || (currentModel.includes('gemini-3') && !currentModel.includes('flash'))) {
                modelConfig.thinkingConfig = { includeThoughts: true, thinkingLevel: "high" };
            }

            if (cachedContentName && currentModel === targetModel) {
                modelConfig.cachedContent = { name: cachedContentName };
            } else {
                modelConfig.systemInstruction = systemInstruction;
            }

            if (currentModel !== targetModel && modelConfig.cachedContent) {
                delete modelConfig.cachedContent;
                modelConfig.systemInstruction = systemInstruction;
                modelConfig.model = currentModel;
            }

            const model = genAI.getGenerativeModel(modelConfig, { apiVersion: 'v1beta' });

            // [DEBUG] Log History Size
            console.log(`[GeminiStream] History Size: ${history.length} messages.`);

            // History Filtering
            let effectiveHistory = history;
            // [Adjustment] Increase Context Window from 5 to 15 messages (approx 7 turns)
            // 5 messages (2.5 turns) is too short for immediate context even with summary.
            // 15 messages provides a better buffer for "recent conversation".
            effectiveHistory = history.slice(-15);
            console.log(`[GeminiStream] Formatted History. Using last ${effectiveHistory.length} messages.`);

            // [Optimization] Gemini SDK requires alternating roles. 
            let processedHistory = effectiveHistory.map(msg => ({
                role: msg.role === 'user' ? 'user' : 'model',
                parts: [{ text: msg.text || "..." }],
            }));

            // Only prepend dummy if strictly necessary
            if (processedHistory.length > 0 && processedHistory[0].role === 'model') {
                processedHistory = [{ role: 'user', parts: [{ text: "(Context Continuation)" }] }, ...processedHistory];
            }

            // [PERF] Log prompt sizes for diagnosis
            const historyChars = processedHistory.reduce((sum, m) => sum + (m.parts[0]?.text?.length || 0), 0);
            console.log(`[GeminiStreamâ±ï¸] Prompt sizes â€” System: ${systemInstruction.length} chars, History: ${historyChars} chars (${processedHistory.length} msgs), ThinkingConfig: ${JSON.stringify(modelConfig.thinkingConfig || 'NONE')}, Cache: ${cachedContentName ? 'YES' : 'NO'}`);

            const chatSession = model.startChat({ history: processedHistory });
            const finalUserMessage = `${dynamicPrompt}\n\n${userMessage}`;

            // STREAMING CALL (with retry for transient API failures)
            console.log(`[GeminiStreamâ±ï¸] Sending message to API... (Dynamic+User Msg: ${finalUserMessage.length} chars)`);
            const streamStartTime = Date.now();
            const result = await retryWithBackoff(
                () => chatSession.sendMessageStream(finalUserMessage),
                2,       // 2 retries (3 total attempts)
                2000,    // Initial delay 2s
                `Stream Story Generation (${currentModel})`
            );
            const streamObjTime = Date.now() - streamStartTime;
            console.log(`[GeminiStreamâ±ï¸] Stream object received in ${streamObjTime}ms. Iterating chunks...`);

            let accumulatedText = "";
            let accumulatedThinkingChars = 0;
            let firstChunkReceived = false;
            let firstTextChunkTime = 0;
            let thinkingEndTime = 0;
            let lastThinkingChunkTime = 0;
            let chunkCount = 0;
            let thinkingChunkCount = 0;
            let textChunkCount = 0;

            for await (const chunk of result.stream) {
                chunkCount++;
                const now = Date.now();
                if (!firstChunkReceived) {
                    const ttft = now - streamStartTime;
                    console.log(`[GeminiStreamâ±ï¸] ğŸš€ FIRST CHUNK after ${ttft}ms (TTFT). Type: ${(chunk.candidates?.[0]?.content?.parts?.[0] as any)?.thought ? 'THINKING' : 'TEXT'}`);
                    firstChunkReceived = true;
                }

                let chunkText = '';
                try {
                    chunkText = chunk.text();
                } catch (e) {
                    // No text in this chunk (likely a thinking chunk)
                }

                // Handle thoughts if present but no text
                if (chunk.candidates && chunk.candidates.length > 0) {
                    const parts = chunk.candidates[0].content?.parts || [];
                    for (const p of parts as any[]) {
                        if (p.thought && p.text) {
                            thinkingChunkCount++;
                            accumulatedThinkingChars += p.text.length;
                            lastThinkingChunkTime = now;
                            yield `<Thinking>${p.text}</Thinking>`;
                        } else if (p.thought) {
                            console.log(`[GeminiStream] Found thought part with no text:`, JSON.stringify(p));
                        }
                    }
                }

                if (chunkText) {
                    textChunkCount++;
                    if (textChunkCount === 1) {
                        firstTextChunkTime = now;
                        // If there was thinking, log the transition
                        if (thinkingChunkCount > 0) {
                            thinkingEndTime = lastThinkingChunkTime;
                            const thinkingDuration = thinkingEndTime - streamStartTime;
                            console.log(`[GeminiStreamâ±ï¸] ğŸ§  THINKING phase ended. Duration: ${thinkingDuration}ms, Chunks: ${thinkingChunkCount}, Chars: ${accumulatedThinkingChars}`);
                        }
                        const ttFirstText = now - streamStartTime;
                        console.log(`[GeminiStreamâ±ï¸] âœï¸ FIRST TEXT CHUNK after ${ttFirstText}ms (Time-to-First-Text)`);
                    }
                    accumulatedText += chunkText;
                    yield chunkText; // Yield partial text
                }
            }

            const streamEndTime = Date.now();
            const response = await result.response; // Wait for full completion to get metadata

            // ===== [PERF SUMMARY] =====
            const totalStreamDuration = streamEndTime - streamStartTime;
            const textGenerationDuration = firstTextChunkTime > 0 ? (streamEndTime - firstTextChunkTime) : 0;
            const thinkingDuration = thinkingEndTime > 0 ? (thinkingEndTime - streamStartTime) : 0;
            const usage = response.usageMetadata;
            const promptTokens = usage?.promptTokenCount || 0;
            const candidateTokens = usage?.candidatesTokenCount || 0;
            const cachedTokens = usage?.cachedContentTokenCount || 0;
            const thinkingTokens = (usage as any)?.thoughtsTokenCount || 0;

            console.log(`\n[GeminiStreamâ±ï¸] ====== PERFORMANCE REPORT ======`);
            console.log(`  Model: ${currentModel}`);
            console.log(`  Cache: ${cachedContentName ? 'HIT' : 'MISS'} (lookup: ${cacheDuration}ms)`);
            console.log(`  Prompt Tokens: ${promptTokens} (cached: ${cachedTokens}, fresh: ${promptTokens - cachedTokens})`);
            console.log(`  Output Tokens: ${candidateTokens} (thinking: ${thinkingTokens}, text: ${candidateTokens - thinkingTokens})`);
            console.log(`  ---`);
            console.log(`  Stream Object: ${streamObjTime}ms`);
            console.log(`  Thinking Phase: ${thinkingDuration}ms (${thinkingChunkCount} chunks, ${accumulatedThinkingChars} chars)`);
            console.log(`  Text Generation: ${textGenerationDuration}ms (${textChunkCount} chunks, ${accumulatedText.length} chars)`);
            console.log(`  Total Stream: ${totalStreamDuration}ms`);
            if (candidateTokens > 0 && totalStreamDuration > 0) {
                const tokensPerSec = (candidateTokens / (totalStreamDuration / 1000)).toFixed(1);
                console.log(`  Throughput: ${tokensPerSec} tokens/sec`);
            }
            console.log(`  ================================\n`);

            // [DEBUG] Log finishReason and safety diagnostics for empty responses
            const finishReason = response.candidates?.[0]?.finishReason;
            const safetyRatings = response.candidates?.[0]?.safetyRatings;
            console.log(`[GeminiStream] Stream Complete. Chunks: ${chunkCount}, AccumulatedText: ${accumulatedText.length} chars, FinishReason: ${finishReason || 'UNKNOWN'}`);

            if (finishReason === 'SAFETY') {
                console.error(`[GeminiStream] âŒ SAFETY FILTER BLOCKED. Ratings:`, JSON.stringify(safetyRatings));
                throw new Error(`Story blocked by safety filter (finishReason=SAFETY). Model: ${currentModel}`);
            }

            if (finishReason === 'RECITATION') {
                console.error(`[GeminiStream] âŒ RECITATION BLOCK.`);
                throw new Error(`Story blocked by recitation filter (finishReason=RECITATION). Model: ${currentModel}`);
            }

            // [FIX] Empty Response Detection & Retry Trigger
            if (!accumulatedText || accumulatedText.trim().length === 0) {
                console.error(`[GeminiStream] âŒ EMPTY RESPONSE. FinishReason: ${finishReason}, Chunks: ${chunkCount}, CandidatesTokenCount: ${response.usageMetadata?.candidatesTokenCount || 0}`);
                // Instead of silently returning empty, throw to trigger fallback model or retry
                throw new Error(`Story generation returned empty text. FinishReason: ${finishReason}, Model: ${currentModel}`);
            }

            // Yield Final Metadata Object as the last item
            yield {
                usageMetadata: response.usageMetadata,
                systemPrompt: systemInstruction,
                finalUserMessage: finalUserMessage,
                usedModel: currentModel,
                totalTokenCount: (promptTokens || 0) + (candidateTokens || 0)
            };

            return; // Success, exit loop
        } catch (error: any) {
            console.warn(`Stream Story Model ${currentModel} failed:`, error.message);
            lastError = error;
        }
    }
    throw lastError || new Error("All stream story models failed.");
}

// Logic Model: Gemini 2.5 Flash (Optimized for speed & JSON)
export async function generateGameLogic(
    apiKey: string,
    lastUserMessage: string,
    lastAiResponse: string,
    gameState: any // [CHANGED] Accepts full GameState to generate shared static prompt
) {
    if (!apiKey) throw new Error('API Key is missing');

    console.log(`[GeminiLogic] GENERATE GAME LOGIC CALLED for GameID: ${gameState.activeGameId}`);

    // [REFACTOR] Event System Decoupled to Parallel Agent
    // Events are no longer processed inside the monolithic Logic Model loop.
    // They are handled by AgentOrchestrator -> EventManager in Phase 2.
    // validEvents is kept empty here to minimize token usage for the Logic Model.
    let validEvents: any[] = [];

    const genAI = new GoogleGenerativeAI(apiKey);

    // [CONTEXT CACHING] Reuse the SAME static prefix
    const staticPrompt = await PromptManager.getSharedStaticContext(gameState);

    // Logic uses MODEL_CONFIG.LOGIC (Optimized for speed)
    const modelsToTry = [MODEL_CONFIG.LOGIC, MODEL_CONFIG.PRE_LOGIC];

    for (const modelName of modelsToTry) {
        try {
            console.log(`Trying logic model: ${modelName}`);

            // [NEW] Static Prompt for Context Caching
            // [NEW] Static Prompt for Context Caching
            // [OPTIMIZATION] Convert JSON to YAML-like text using LoreConverter to save tokens & match Story Model format
            const rankCriteria = gameState.lore?.levels ? LoreConverter.convertMartialArtsLevels(gameState.lore.levels) : null;
            const romanceGuide = gameState.lore?.romance_guide ? LoreConverter.convertRomance(gameState.lore.romance_guide) : null;
            const combatGuide = gameState.lore?.combat_guide ? LoreConverter.convertCombat(gameState.lore.combat_guide) : null;

            const staticLogicPrompt = getStaticLogicPrompt(gameState.activeGameId, rankCriteria, romanceGuide, combatGuide);

            // [NOTE] Logic Model is usually too small for Context Caching (<32k)
            // So we skip the getOrUpdateCache step here to save latency. 
            // If Logic contexts ever get huge, use the same pattern as generateResponse.

            const model = genAI.getGenerativeModel({
                model: modelName,
                safetySettings,
                generationConfig: { responseMimeType: "application/json" },
                systemInstruction: staticLogicPrompt // [OPTIMIZATION] Cache the Rules & Format
            });

            // Logic Prompt (Dynamic Suffix)
            // Note: We need to pass pruned stats for logic processing logic, but PromptManager uses full state.
            // Ideally, we prune stats separately.
            const stats = gameState.playerStats || {};
            // Prune huge arrays to save tokens in the Dynamic part (Static part is already big but cached)
            const {
                characterData,
                availableBackgrounds,
                availableCharacterImages,
                availableExtraImages,
                chatHistory, // [Optimize] Remove history from stats dumping (Passed separately as strings)
                displayHistory,
                worldData: _unusedWorldData, // [FIX] Exclude worldData from prunedStats (Renamed to avoid conflict)
                lore, // [CRITICAL FIX] Exclude Lore from Logic Model (It uses huge tokens and isn't needed for logic)
                wikiData, // [OPTIMIZATION] Exclude Wiki Data (Generic info not needed for step logic)
                scriptQueue, // [OPTIMIZATION] Exclude Visual Novel Script Queue
                textMessageHistory, // [OPTIMIZATION] Exclude SMS History
                constants, // [OPTIMIZATION] Exclude Static Constants
                events: _ignoredEvents, // [OPTIMIZATION] Exclude Raw Events List (Passed filtered)
                // characterMap removed (no longer needed)
                // extraMap removed (no longer needed)
                characterCreationQuestions,
                ...prunedStats
            } = gameState;

            // [NEW] Inject computed scenarioContext into prunedStats for prompt templates
            const mem = gameState.scenarioMemory;
            if (mem) {
                const parts: string[] = [];
                if (mem.tier2Summaries?.length > 0) {
                    parts.push('[ì¥ê¸° ê¸°ì–µ]\n' + mem.tier2Summaries.join('\n---\n'));
                }
                if (mem.tier1Summaries?.length > 0) {
                    parts.push('[ìµœê·¼ ì¤„ê±°ë¦¬]\n' + mem.tier1Summaries.join('\n---\n'));
                }
                (prunedStats as any).scenarioContext = parts.join('\n\n') || 'ê²Œì„ ì‹œì‘';
            } else {
                (prunedStats as any).scenarioContext = 'ê²Œì„ ì‹œì‘';
            }

            const worldData = gameState.worldData || { locations: {}, items: {} }; // Fallback to empty if missing

            // Get lightweight context for Logic Model
            const logicContext = PromptManager.getLogicModelContext(gameState);

            // [NEW] Dynamic Prompt (Stats & Action)
            const prompt = getDynamicLogicPrompt(
                prunedStats,
                lastUserMessage,
                lastAiResponse,
                logicContext,
                worldData,
                validEvents // [NEW] Pass filtered events to logic prompt
            );

            // [DEBUG LOG] Verify Logic Model Prompt Size
            console.log("--- [Logic Model Input Prompt] ---");
            console.log(prompt);
            console.log("----------------------------------");

            // [Retry Wrapper] logic generation
            const result = await retryWithBackoff(
                () => model.generateContent(prompt),
                3,
                1500,
                `Logic Generation (${modelName})`
            );
            const response = result.response;
            const text = response.text();

            console.log("Raw Logic Response:", text); // Debug Log

            try {
                const json = JSON.parse(text);
                console.log("Parsed Logic JSON:", json); // Debug Log

                // Attach usage metadata if available
                if (response.usageMetadata) {
                    json._usageMetadata = response.usageMetadata;
                }

                // Attach debug info
                json._debug_prompt = prompt;
                json._debug_static_prompt = staticLogicPrompt;
                json._debug_raw_response = text;

                // [REFACTOR] Event Trigger Logic Moved
                // Logic Model no longer selects events.
                // EventManager (Phase 2 Parallel) handles selection and Next State Injection.
                if (json.triggerEventId) {
                    console.warn(`[GeminiLogic] DEPRECATED: Logic Model returned triggerEventId '${json.triggerEventId}' but Event System is now parallel.`);
                }

                return json;
            } catch (e) {
                console.error("Failed to parse logic JSON:", e);
                // Try to extract JSON if it's wrapped in markdown code blocks
                const match = text.match(/```json([\s\S]*?)```/);
                if (match) {
                    try {
                        const json = JSON.parse(match[1]);
                        console.log("Parsed Logic JSON (from markdown):", json); // Debug Log

                        // Attach debug info
                        json._debug_prompt = prompt;
                        json._debug_static_prompt = staticLogicPrompt;
                        json._debug_raw_response = text;

                        return json;
                    } catch (e2) {
                        console.error("Failed to parse extracted JSON:", e2);
                    }
                }
                // If parsing failed, continue to next model (maybe model output was bad)
                throw new Error("JSON parsing failed");
            }

        } catch (error: any) {
            console.warn(`Logic Model ${modelName} failed:`, error.message || error);
            // Continue to next model
        }
    }

    return null;
}

// [NEW] Casting Logic Model: Gemini 2.5 Flash
export async function generateCastingDecision(
    apiKey: string,
    context: {
        location: string;
        summary: string;
        userInput: string;
        activeCharacters: string[];
    },
    candidates: { id: string; name: string; reasons: string[]; score: number }[],
    bgCandidates: { id: string; name: string; reasons: string[]; score: number }[],
    modelName: string = MODEL_CONFIG.LOGIC
) {
    if (!apiKey) throw new Error('API Key is missing');
    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({
        model: modelName,
        safetySettings,
        generationConfig: { responseMimeType: "application/json" }
    });

    const prompt = `
    ë‹¹ì‹ ì€ ë¹„ì£¼ì–¼ ë…¸ë²¨ ê²Œì„ì˜ 'ìºìŠ¤íŒ… ë””ë ‰í„°'ì…ë‹ˆë‹¤.
    í˜„ì¬ ì¥ë©´ì˜ ê¸´ì¥ê°, ì¬ë¯¸, ì„œì‚¬ì  ê°œì—°ì„±ì„ ê³ ë ¤í•˜ì—¬ **ê°€ì¥ ì ì ˆí•œ ë“±ì¥ì¸ë¬¼**ì„ ì„ ë³„í•´ì•¼ í•©ë‹ˆë‹¤.

    [í˜„ì¬ ìƒí™©]
    - ì¥ì†Œ: ${context.location}
    - ì´ì „ ì¤„ê±°ë¦¬: ${context.summary}
    - í”Œë ˆì´ì–´ í–‰ë™: "${context.userInput}"
    - í˜„ì¬ í™”ë©´ì— ìˆëŠ” ì¸ë¬¼: ${JSON.stringify(context.activeCharacters)}

    [í›„ë³´ ëª…ë‹¨ (ì ìˆ˜ìˆœ ì •ë ¬ë¨)]
    ${JSON.stringify(candidates.map(c => ({ id: c.id, name: c.name, score: c.score.toFixed(1), reasons: c.reasons })))}
    
    [ë°°ê²½ ì¸ë¬¼ í›„ë³´ (ì ìˆ˜ìˆœ)]
    ${JSON.stringify(bgCandidates.map(c => ({ id: c.id, name: c.name, score: c.score.toFixed(1), reasons: c.reasons })))}

    [ì§€ì‹œì‚¬í•­]
    1. **Active ì„ ë°œ (ìµœëŒ€ 6ëª…)**: ëŒ€í™”ë‚˜ ì‚¬ê±´ì˜ ì£¼ì²´ê°€ ë  ì¸ë¬¼ì„ ë½‘ìœ¼ì„¸ìš”.
       - ì ìˆ˜ê°€ ë†’ë”ë¼ë„, ì§€ê¸ˆ ë¶„ìœ„ê¸°ì— ë§ì§€ ì•Šê±°ë‚˜ ë„ˆë¬´ ëœ¬ê¸ˆì—†ìœ¼ë©´ ì œì™¸í•˜ì„¸ìš”.
       - ë°˜ëŒ€ë¡œ ì ìˆ˜ê°€ ë‚®ë”ë¼ë„, **"ì§€ê¸ˆ ë“±ì¥í•˜ë©´ ê°ˆë“±ì´ ê³ ì¡°ë˜ê±°ë‚˜ ì¬ë¯¸ìˆê² ë‹¤"** ì‹¶ì€ ì¸ë¬¼ì€ ê³¼ê°íˆ ë°œíƒí•˜ì„¸ìš”.
       - ì£¼ì¸ê³µì´ ì§ì ‘ ë¶€ë¥¸ ì¸ë¬¼ì€ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
       - **ì¤‘ìš”**: ê° Active ì¸ë¬¼ì— ëŒ€í•´ "ì™œ ì§€ê¸ˆ ë“±ì¥í•˜ë©°, ë¬´ì—‡ì„ í•˜ëŠ”ì§€"ì— ëŒ€í•œ ì§§ì€ **[ë“±ì¥ ì‹œë‚˜ë¦¬ì˜¤]**ë¥¼ ì œì•ˆí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨ìˆœ ë“±ì¥ì´ ì•„ë‹ˆë¼, ì‚¬ê±´ì´ë‚˜ ëŒ€í™”ë¥¼ ìœ ë°œí•˜ëŠ” êµ¬ì²´ì ì¸ í–‰ë™ì„ ì ìœ¼ì„¸ìš”.
    2. **Background ì„ ë°œ (ìµœëŒ€ 12ëª…)**: ë°°ê²½ì— ì„œ ìˆì„ ì¸ë¬¼ì„ ë½‘ìœ¼ì„¸ìš”.
       - ì¥ì†Œì— ì–´ìš¸ë¦¬ëŠ” ì¸ë¬¼(ì£¼ì¸, ì§ì› ë“±) ìš°ì„ .
       - Active ì¸ë¬¼ì˜ ì§€ì¸/ë¶€í•˜ ë“±.

    [Output Format (JSON)]
    {
        "active": [
            { "id": "char_id_1", "scenario": "Brief description of arrival action (e.g. 'Suddenly bursts in kicking the door')." },
            { "id": "char_id_2", "scenario": "Quietly observing from corner." }
        ],
        "background": ["id3", "id4", "id5"],
        "reason": "Brief explanation of why this casting was chosen."
    }
    `;

    try {
        const result = await retryWithBackoff(
            () => model.generateContent(prompt),
            2,
            1000,
            `Casting Decision (${modelName})`
        );
        const text = result.response.text();
        console.log(`[GeminiCasting] Raw AI Decision: ${text.substring(0, 100)}...`);
        return JSON.parse(text);
    } catch (e: any) {
        console.warn("AI Casting Failed:", e.message);
        return null; // Fallback to heuristic
    }
}

// [LEGACY] Memory Summarization - kept for backward compat, delegates to Tier1
export async function generateSummary(
    apiKey: string,
    currentSummary: string,
    recentDialogue: Message[]
) {
    return generateTier1Summary(apiKey, recentDialogue);
}

// [NEW] Tier 1 Summary: 10-turn chunk â†’ 2000 chars
export async function generateTier1Summary(
    apiKey: string,
    recentDialogue: Message[]
): Promise<string> {
    if (!apiKey) return '';

    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({
        model: MODEL_CONFIG.SUMMARY,
        safetySettings
    });

    const dialogueText = recentDialogue.map(msg =>
        `${msg.role === 'user' ? 'Player' : 'AI'}: ${msg.text}`
    ).join('\n');

    const prompt = `
    ë‹¹ì‹ ì€ ë°©ëŒ€í•œ ì„œì‚¬ë¥¼ í•µì‹¬ë§Œ ì¶”ë ¤ ê¸°ë¡í•˜ëŠ” 'ì™•ì‹¤ ì„œê¸°'ì…ë‹ˆë‹¤.
    ìµœê·¼ 10í„´ì˜ ëŒ€í™”ë¥¼ ì½ê³ , í•µì‹¬ ì‚¬ê±´ë§Œ ìš”ì•½í•˜ì‹­ì‹œì˜¤.

    [ìµœê·¼ ëŒ€í™”]
    ${dialogueText}

    [ì‘ì„± ì§€ì¹¨]
    1. **ë…ë¦½ì  ìš”ì•½**: ì´ ëŒ€í™” êµ¬ê°„ì—ì„œ ì¼ì–´ë‚œ ì‚¬ê±´ë§Œ ìš”ì•½í•˜ì‹­ì‹œì˜¤. ê³¼ê±° ë§¥ë½ì€ ë³„ë„ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
    2. **ë¶„ëŸ‰ ì œí•œ**: ë°˜ë“œì‹œ **2000ì(ê³µë°± í¬í•¨)** ì´ë‚´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    3. **í˜„ì¬ì„±**: ì´ êµ¬ê°„ì´ ëë‚  ë•Œ í”Œë ˆì´ì–´ì˜ ìœ„ì¹˜, ë™í–‰ì¸, ìƒíƒœë¥¼ ëª…ì‹œí•˜ì‹­ì‹œì˜¤.
    4. **í˜•ì‹**: ì¤„ê¸€, ê°„ê²°í•œ ë¬¸ì¥.

    [í•„ìˆ˜ í¬í•¨ í•­ëª©]
    - ì´ êµ¬ê°„ì—ì„œ ë°œìƒí•œ ì£¼ìš” ì‚¬ê±´
    - ìœ„ì¹˜ ë³€ê²½
    - ì¸ë¬¼ ê´€ê³„ ë³€í™”
    - íšë“/ì†Œì‹¤í•œ ì•„ì´í…œ
    - ë‹¹ë©´í•œ ëª©í‘œ ë³€í™”

    [ìš”ì•½]:
    `;

    try {
        const result = await model.generateContent(prompt);
        const response = result.response;
        const text = response.text();
        console.log("[Tier1 Summary] Generated:", text.substring(0, 80) + "...");
        return text.slice(0, 2000); // Hard cap
    } catch (error: any) {
        console.warn("[Tier1 Summary] Failed:", error.message || error);
        return '';
    }
}

// [NEW] Tier 2 Summary: Compress 10 Tier1 summaries â†’ 5000 chars
export async function generateTier2Summary(
    apiKey: string,
    tier1Summaries: string[]
): Promise<string> {
    if (!apiKey) return '';

    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({
        model: MODEL_CONFIG.SUMMARY,
        safetySettings
    });

    const summaryText = tier1Summaries.map((s, i) =>
        `[êµ¬ê°„ ${i + 1}]\n${s}`
    ).join('\n\n');

    const prompt = `
    ë‹¹ì‹ ì€ ë°©ëŒ€í•œ ì„œì‚¬ë¥¼ í•µì‹¬ë§Œ ì¶”ë ¤ ê¸°ë¡í•˜ëŠ” 'ì™•ì‹¤ ì„œê¸°'ì…ë‹ˆë‹¤.
    ì•„ë˜ 10ê°œì˜ êµ¬ê°„ë³„ ìš”ì•½ì„ ì½ê³ , í•˜ë‚˜ì˜ ëŒ€ì„œì‚¬ ìš”ì•½ìœ¼ë¡œ ì••ì¶•í•˜ì‹­ì‹œì˜¤.

    [êµ¬ê°„ë³„ ìš”ì•½ë“¤]
    ${summaryText}

    [ì‘ì„± ì§€ì¹¨]
    1. **ì••ì¶•**: 10ê°œ êµ¬ê°„ì˜ ì‚¬ê±´ì„ í•˜ë‚˜ì˜ ì—°ì†ëœ ì´ì•¼ê¸°ë¡œ í†µí•©í•˜ì‹­ì‹œì˜¤.
    2. **ë¶„ëŸ‰ ì œí•œ**: ë°˜ë“œì‹œ **5000ì(ê³µë°± í¬í•¨)** ì´ë‚´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    3. **ì‹œê°„ ìˆœì„œ**: ê°€ì¥ ì˜¤ë˜ëœ ì‚¬ê±´ë¶€í„° ìµœê·¼ ìˆœìœ¼ë¡œ ì„œìˆ í•˜ë˜, ì˜¤ë˜ëœ ì‚¬ê±´ì¼ìˆ˜ë¡ ê³¼ê°íˆ ì¶•ì•½í•˜ì‹­ì‹œì˜¤.
    4. **í•µì‹¬ ìœ ì§€**: ì¸ë¬¼ ê´€ê³„, ì£¼ìš” ì „íˆ¬, ì„±ì¥ ì´ë²¤íŠ¸, ì†Œì† ë³€í™” ë“± ì¥ê¸°ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì‚¬ê±´ë§Œ ë‚¨ê¸°ì‹­ì‹œì˜¤.
    5. **í˜•ì‹**: ì¤„ê¸€, ë‹¨ë½ êµ¬ë¶„.

    [í†µí•© ìš”ì•½]:
    `;

    try {
        const result = await model.generateContent(prompt);
        const response = result.response;
        const text = response.text();
        console.log("[Tier2 Summary] Generated:", text.substring(0, 80) + "...");
        return text.slice(0, 5000); // Hard cap
    } catch (error: any) {
        console.warn("[Tier2 Summary] Failed:", error.message || error);
        return '';
    }
}

// Memory Summarization for Characters: Use Configured Model (Gemini 2.5 Flash)
export async function generateCharacterMemorySummary(
    apiKey: string,
    characterName: string,
    existingMemories: string[]
): Promise<string[]> {
    if (!apiKey || existingMemories.length <= 10) return existingMemories;

    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({
        model: MODEL_CONFIG.SUMMARY, // Logic/Flash Model
        safetySettings
    });

    const memoryText = existingMemories.map(m => `- ${m}`).join('\n');

    const prompt = `
    ë‹¹ì‹ ì€ RPG ê²Œì„ì˜ 'ìºë¦­í„° ê¸°ì–µ ê´€ë¦¬ì'ì…ë‹ˆë‹¤.
    í˜„ì¬ [${characterName}] ìºë¦­í„°ì˜ ê¸°ì–µì´ ë„ˆë¬´ ë§ì•„ì ¸ì„œ ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    ìì˜í•œ ê¸°ì–µë“¤ì„ ë³‘í•©í•˜ê³ , ì¤‘ìš”í•œ ì‚¬ê±´ ìœ„ì£¼ë¡œ ìš”ì•½í•˜ì—¬ **10ê°œ ì´í•˜ì˜ í•µì‹¬ ê¸°ì–µ ë¦¬ìŠ¤íŠ¸**ë¡œ ì¬êµ¬ì„±í•˜ì‹­ì‹œì˜¤.

    [ì…ë ¥ëœ ê¸°ì–µ ëª©ë¡]
    ${memoryText}

    [ì‘ì„± ì§€ì¹¨]
    1. **ì¤‘ìš”ë„ ê¸°ë°˜ ì„ ë³„**: ì•½ì†, ì›í•œ, ìƒëª…ì˜ ì€ì¸, ì£¼ìš” í€˜ìŠ¤íŠ¸ ê´€ë ¨ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë‚¨ê¸°ì‹­ì‹œì˜¤.
    2. **ë³‘í•©**: "ê°™ì´ ë°¥ì„ ë¨¹ìŒ", "ê°™ì´ ì°¨ë¥¼ ë§ˆì‹¬" -> "í”Œë ˆì´ì–´ì™€ ì‹ì‚¬ë¥¼ í•˜ë©° ì¹œí•´ì§" ì²˜ëŸ¼ ë¹„ìŠ·í•œ ì‚¬ê±´ì€ í•˜ë‚˜ë¡œ í•©ì¹˜ì‹­ì‹œì˜¤.
    3. **ìµœì‹ ì„± ìœ ì§€**: ê°€ì¥ ìµœê·¼ì— ë°œìƒí•œ ì¤‘ìš”í•œ ë³€í™”ëŠ” ìƒì„¸íˆ ê¸°ë¡í•˜ì‹­ì‹œì˜¤.
    4. **í˜•ì‹**: JSON String Array ("[...]").

    [ìš”ì•½ëœ ê¸°ì–µ ë¦¬ìŠ¤íŠ¸ (JSON Array)]:
    `;

    try {
        const result = await model.generateContent(prompt);
        const response = result.response;
        const text = response.text();

        // JSON Parsing Attempt
        const firstBracket = text.indexOf('[');
        const lastBracket = text.lastIndexOf(']');

        if (firstBracket !== -1 && lastBracket !== -1) {
            const jsonText = text.substring(firstBracket, lastBracket + 1);
            const parsed = JSON.parse(jsonText);
            if (Array.isArray(parsed)) {
                console.log(`[MemorySummary] Summarized ${characterName}'s memories: ${existingMemories.length} -> ${parsed.length}`);
                return parsed;
            }
        }

        throw new Error("Failed to parse JSON response");

    } catch (error: any) {
        console.warn(`[MemorySummary] Failed to summarize memories for ${characterName}:`, error);
        // Fallback: Just keep the last 10 if AI fails
        return existingMemories.slice(-10);
    }
}

// [Startup Warmup]
// Fire-and-forget request to create/warm the cache.
export async function preloadCache(apiKey: string, initialState: any) {
    if (!apiKey) return;
    try {
        console.log("[Gemini] Pre-loading cache...");

        // 1. Get the Exact Same Static Prompt as normal requests
        const staticPrompt = await PromptManager.getSharedStaticContext(initialState, initialState.activeChars, initialState.spawnCandidates);

        // [CACHE WARMUP FIX]
        // Instead of dummy generating, we should try to create the cache object itself here if possible.
        // But for consistency, let's just trigger the normal flow or call getOrUpdateCache directly.
        // We need 'modelName' which is MODEL_CONFIG.STORY.

        // This won't actually "warm" the model inferencing, but it might create the cache entry.
        const modelName = MODEL_CONFIG.STORY;
        const overrideKey = initialState.personaOverride ? `_PERSONA_${initialState.personaOverride}` : '';
        const cacheDisplayName = `CACHE_${initialState.activeGameId}_STORY_v1.3${overrideKey}`;

        if (staticPrompt.length > 50000) {
            await getOrUpdateCache(apiKey, cacheDisplayName, staticPrompt, modelName);
            console.log("[Gemini] Cache Entry Verified/Created during warmup.");
        }

        // 2. Configure Model
        // MATCH THE MAIN MODEL: MODEL_CONFIG.STORY
        const genAI = new GoogleGenerativeAI(apiKey);
        const model = genAI.getGenerativeModel({
            model: MODEL_CONFIG.STORY,
            systemInstruction: staticPrompt,
        }, {
            apiVersion: 'v1beta',
        });

        // 3. Send a dummy prompt
        const result = await model.generateContent("System Initialization (Warmup)");

        console.log("[Gemini] Cache Warmup Request Sent!");
        return result.response.usageMetadata;
    } catch (e) {
        console.error("[Gemini] Cache Warmup Failed:", e);
        return null;
    }
}

// =========================================================
// Turn Orchestration Logic
// =========================================================

const SUMMARY_THRESHOLD = 3;

export async function handleGameTurn(
    apiKey: string,
    state: any,             // State including turnCount and summary
    history: Message[],     // UI display history
    userInput: string
) {
    console.log(`[handleGameTurn] Turn: ${state.turnCount}, Memory Tier1: ${state.scenarioMemory?.tier1Summaries?.length || 0
        }, Tier2: ${state.scenarioMemory?.tier2Summaries?.length || 0}`);

    // 1. Add user message to a temporary history for the AI call
    // Note: The caller updates the actual UI history. This is for the AI logic.
    // If we want to strictly follow the user's snippet, we operate on the list passed.
    const newHistory = [...history, { role: 'user', text: userInput } as Message];

    // 2. Main Story Model
    // State should already have the summary injected via PromptManager in generateResponse
    const storyResult = await generateResponse(
        apiKey,
        history,
        userInput,
        state,
        'ko' // Default to Korean as per context
    );

    // 3. Game Logic Model
    const logicResult = await generateGameLogic(
        apiKey,
        userInput,
        storyResult.text,
        state
    );

    // Merge logic result into nextState (Conceptually)
    // The actual state update depends on the caller (client or server action) applying the JSON.
    // But for the purpose of summarization, we need to know the *next* turn count.

    let nextState = { ...state, ...logicResult };

    // [New] Process Time & Survival Logic
    const currentStats = nextState.playerStats || {};
    let currentFatigue = currentStats.fatigue || 0;

    // 1. Fatigue Update from Logic
    if (logicResult.fatigueChange) {
        currentFatigue = Math.max(0, Math.min(100, currentFatigue + logicResult.fatigueChange));
    }

    // 2. Time & Sleep Logic
    let currentDay = nextState.day || 1;
    let currentTime = nextState.time || 'Morning';
    const timePhases = ['Morning', 'Afternoon', 'Evening', 'Night'];

    if (logicResult.isSleep) {
        // Sleep Action
        if (currentTime === 'Night') {
            // Full Rest: Next Day
            currentDay++;
            currentTime = 'Morning';
            currentFatigue = 0; // Reset Fatigue
        } else {
            // Nap: Reduce Fatigue, Advance Time (1 phase)
            currentFatigue = Math.max(0, currentFatigue - 30);
            const nextIdx = (timePhases.indexOf(currentTime) + 1) % 4;
            if (nextIdx === 0) currentDay++;
            currentTime = timePhases[nextIdx];
        }
    } else {
        // Normal Time Progression (Support both new 'timeConsumed' and legacy 'timeProgress')
        const timeAdvance = logicResult.timeConsumed || (logicResult.timeProgress ? 1 : 0);

        if (timeAdvance > 0) {
            let idx = timePhases.indexOf(currentTime);

            // Advance by N steps
            for (let i = 0; i < timeAdvance; i++) {
                idx++;
                if (idx >= timePhases.length) {
                    idx = 0;
                    currentDay++; // Night -> Morning triggers next day
                }
            }
            currentTime = timePhases[idx];
        }
    }

    // Apply Time/Fatigue Updates to State
    nextState.day = currentDay;
    nextState.time = currentTime;
    nextState.playerStats = { ...currentStats, fatigue: currentFatigue };

    // Logic result might have specific fields like hpChange, etc. 
    // We are interested in 'turnCount' and 'scenarioMemory'.
    // The user's snippet implies we handle turn counting here or assume it's passed.
    // If the state passed in is the *current* state (before this turn), we should increments turn info.

    const currentTurnCount = (state.turnCount || 0) + 1;

    // [LEGACY] Summary logic - now handled by turn-based trigger in VisualNovelUI
    // Keeping minimal compat for any direct callers
    let newSummary = '';

    if (currentTurnCount >= 15 && (currentTurnCount - 15) % 10 === 0) {
        console.log(`[handleGameTurn] Triggering Tier1 Summary at turn ${currentTurnCount}...`);

        const fullDialogue = [...newHistory, { role: 'model', text: storyResult.text } as Message];
        const recentDialogue = fullDialogue.slice(-20);

        newSummary = await generateSummary(
            apiKey,
            '',
            recentDialogue
        );

        console.log("[handleGameTurn] New Tier1 Summary Generated.");
    }

    // Calculate Total Cost
    let totalCost = 0;

    // 1. Story Cost
    if (storyResult.usageMetadata) {
        // [FIX] We need to report the cached tokens to the calculator if they existed
        // Our 'storyResult.totalTokenCount' is a sum, but cost needs specific breakdown.
        // UsageMetadata has 'cachedContentTokenCount' (if supported by backend).
        // Let's ensure we account for it.
        const meta = storyResult.usageMetadata;
        totalCost += calculateCost(
            storyResult.usedModel || MODEL_CONFIG.STORY,
            meta.promptTokenCount || 0,
            meta.candidatesTokenCount || 0,
            meta.cachedContentTokenCount || 0
        );
    }

    // 2. Logic Cost (Assuming LOGIC model)
    if (logicResult._usageMetadata) {
        totalCost += calculateCost(
            MODEL_CONFIG.LOGIC,
            logicResult._usageMetadata.promptTokenCount || 0,
            logicResult._usageMetadata.candidatesTokenCount || 0
        );
    }

    // 3. Summary Cost (Assuming SUMMARY model, if generated)
    // Note: Summary generation doesn't currently return metadata easily, 
    // but it's small. We can ignore or approximate if needed.
    // For perfection, we'd need generateSummary to return metadata.
    // Let's assume 0 for optional summary for now or update generateSummary later.

    return {
        reply: storyResult.text,
        logic: logicResult,
        summary: newSummary,
        turnCount: currentTurnCount,
        storyResult: storyResult,
        cost: totalCost // [New]
    };
}
